
        
   SPYD  
        



Pending releases:
  - eslint-config-standard-prettier-fp
  - gulp-shared-tasks

Store options:
  - pass OPTS.store.STORE.name PACKAGE.name to every store:
     - def: undefined if no packageRoot
  - HTTP store base URL should be /projects/NAME/
     - NAME is --store.http.name, defaulting to OPTS.name

show|remove should target benchmarks grouped by jobs:
  - store.remove(BENCHMARK.id) -> store.remove(BENCHMARK.job)
  - report/main.js should take {benchmarks,job} as input instead of {benchmarks,benchmark}
     - the reported and returned benchmark is searched with `job`
     - the run action pass [...benchmarks, benchmark] and benchmark.job
     - the show action find target benchmark then pass benchmarks and benchmark.job

debug action:
  - separate top-level action
  - separate runner event:
     - run benchmarked function exactly once
     - do not measure it
        - no bias calculation
  - during initial load, stdout|stderr "pipe" + getStream()
     - if error thrown, print stderr then newline then stdout
     - otherwise noop
  - during each iteration, stdout|stderr "inherit"
  - before each iteration, print iteration.name
  - after each iteration, print empty line
  - no benchmarking
     - no std4/std5
     - no timeout
     - no duration printing
  - no progress reporting
  - no reporting
  - no saving/loading

OPTS.limit[.ID...] NUM
  - NUM is max ITERATION.stats.diff
  - can be specified several times
  - ID:
     - looked up as taskId then variationId then commandId
     - can specify several for same group
  - ITERATION.stats.limit NUM:
     - actual max time, not percentage of diff
     - reporter-only, not saved
  - ITERATION.slow BOOL
     - reporter-only, not saved
     - if stats.median >= stats.limit
     - if true:
        - exit code 1
        - no CLI error message

CI:
  - get CI information using library like ci-info
     - set it as several variables on BENCHMARK.system*
        - i.e. env-wise
     - systemPretty combines URL and titles, system doesnt
  - default OPTS.job to CI job ID (using ci-info or other library)
  - try to default OPTS.env to ENVVAR CIs
     - e.g. current matrix envvar full string with Travis

BENCHMARK.git OBJ:
  - git information (commit, branch, etc.)
  - only if `git` installed
  - reporters should not show it unless --system
  - when merging previous benchmarks of same job, validate that if same job + env, git info must be deep equal
  - document --env as being for different git branches too

FILE.version NUM
  - saved data file major version
     - different from spyd PACKAGE.version
  - on store.load(), if old FILE.version, throw error saying "spyd migrate" should be run
  - add "spyd migrate" action that does the migration:
     - add code for each new major release
     - requires store.replace(BENCHMARK_ARR, STORE_OPTS)
  - for reported|returned BENCHMARK (as opposed to saved data), use normal PACKAGE.version major release instead

Add HTTP store:
  - add(): POST / BENCHMARK -> void
  - list(): GET / -> BENCHMARK_ARR
     - must be sorted from oldest to newest (using BENCHMARK.timestamp)
     - allow HTTP caching
  - remove(): DELETE /ID -> void

spyd-runner-cli:
  - benchmark FILE
     - YAML
     - OBJ:
        - variables OBJ:
           - VAR "COMMAND"
        - variations OBJ_ARR:
           - id, title, value: like spyd-runner-node
              - value can be NUM|BOOL too: serialized to STR
        - TASK:
           - title, variations: like spyd-runner-node
           - before|after|main: like spyd-runner-node except as 'COMMAND'
        - shell BOOL
  - 'COMMAND':
     - FILE.shell BOOL:
        - for all 'COMMAND' in file
        - default true if spawning shell does not add too much duration|variance. Otherwise false. Check it.
           - if big difference, do not allow mix two files with different value
     - use execa(..., { shell: true }) (if shell true) or execa.command() (if shell false)
     - any exit code non-0 propagates the error, i.e. aborts run
  - variables:
     - templating on any STR value
        - done before spawning commands, i.e. shell-independent
     - {{VAR}}
        - can escape as {{{VAR}}}
     - can target:
        - ENVVAR
        - OBJ.variables.VAR
           - VAR must be 0-9a-zA-Z-_
        - {{variation}} (FILE.variations[*].value)
        - {{before}} (TASK.before output)
     - processed in order:
        - first FILE.variables, then everything but TASK.before|after|main, then TASK.before, then TASK.main|after
        - FILE.variables are processed even if not used
     - propagate errors (exit code non 0)
  - try to re-use spyd-runner-node where it makes sense:
     - copy the code first, then once runner complete, see what be refactored as shared code
     - check the parts that might not make sense due to measures being probably much slower (check each):
        - nowBias
        - repeat
        - cold starts
     - loopBias should:
        - take into account process spawning
        - use a very fast command, OS-specific (e.g. "true" on Unix)

Competitors benchmark:
  - benchmark with other benchmarking tools
  - each should benchmark Math.random() for the same duration
     - use different durations as variations
  - report both medians (for accuracy) and standard deviation (for precision)

Add progress reporters:
  - spinner with task name and current median
  - progress bar
  - both above (def)

Reporters:
  - types:
     - JSON
     - CLI list
     - CLI table
     - Markdown list
     - Markdown table
     - CLI graphs|histograms
     - CLI where the tasks are in both axis, and the cells are the difference in %
     - CLI with horizontal bars for medians
        - with full CLI width for slowest median
        - still show numbers on top of bars (or on their left side)
        - def reporter instead of simple CLI list, except when there is only one iteration
        - for Markdown too???
     - HTML
     - CLI reporter that shows BENCHMARK.iterations[*].previous as a time series
  - CLI|Markdown list:
     - follow the formatting I used in fast-cartesian example
        - simple list for TASK with no variations
  - CLI|Markdown tables:
     - variations as x axis, tasks as y axis
  - default reporter:
     - CLI|Markdown table if more than half of cells would be filled, and some TASK.variations are defined
        - CLI|Markdown list otherwise
     - Markdown table|list if OPTS.insert '*.md|*.markdown|README|readme'
        - CLI table|list otherwise
  - should use iteration.fastest BOOL for highlighting
  - when BENCHMARK.histogram|percentiles [] reporters should show "Not enough data" (but not throw)
  - Markdown|HTML reporters should add "Benchmarked with [spyd](https://github.com/ehmicky/spyd)" when REPORTER_OPTS.link true
     - but CLI reporters should never add anything (except 'debug' reporter)

Separate each reporter to own package
  - e.g. spyd-report-*
  - builtin reporters are required as production dependencies by core

Separate now.js to its own package

Separate resolution.js to its own package

Separate stats to its own package
  - compare with existing packages

Separate measuring part to its own package
  - child processes might be able to spawn only this instead of the full spyd package

Separate runners into own packages
  - e.g. spy-run-node
  - distributed with npm, JavaScript endpoint, but can otherwise use any programming language

Add other runners:
  - spyd-run-chrome (maybe using puppetter)
  - spyd-run-firefox (maybe using puppetter-firefox)
  - spyd-run-selenium
  - spyd-run-bash

Validation|testing utility to help people creating reporters, runners, progress reporters

Consider lowering the valid Node version for spyd-runner-node, so that `run.node.versions` can target lower versions

Add roadmap:
  - point to it from contribution doc to orient contributors towards features I want (e.g. HTML reporter)

Promote:
  - add keywords (GitHub, package.json)

Features:
  - most precise and accurate benchmarking
  - pretty reporting
  - comparison with previous benchmarks
  - performance testing
  - automatically insert latest benchmarks into your documentation
  - custom reporters
  - TypeScript support

Commercial offer:
  - reporting dashboard:
     - show time series (i.e. keep history)
        - should not lose history when change only the `title` of the function|variant
     - nice data visualization
     - should show function bodies
  - code editor for benchmark files:
     - run benchmarks (inside browser not on our servers)
     - send benchmark results to API (like what users would do on CI otherwise)
     - show results from API
     - i.e. can be used like jsperf
  - Sharing like jsperf:
     - allow users to run in their own browsers
  - PR bot
  - notifications
  - user should report benchmark results to our API
     - like Codecov does
     - i.e. we do not pay for infrastructure cost, except simple CRUD API to store benchmark results
     - should be integrated with CI (i.e. use `ci-info`)
  - pricing:
     - free for open source
     - pay per private repo
