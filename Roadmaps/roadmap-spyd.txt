
        
   SPYD  
        



Reporters:
  - FUNC(BENCHMARK_ARR, REPORTER_OPTS)[->STR]
     - if STR, either printed to console or put in file (if OPTS.replace)
  - OPTS.reporters 'MODULE'_ARR
  - REPORTER_OPTS are OPTS.report.REPORTER
     - --report.REPORTER.* on the CLI, but OBJ programatically and in config file
  - types:
     - silent
     - JSON
     - CLI list
     - CLI table
     - Markdown list
     - Markdown table
     - HTML
  - CLI|Markdown list:
     - follow the formatting I used in fast-cartesian example
        - simple list for TASK with no TASK.parameters
  - CLI|Markdown tables:
     - parameters as x axis, mains as y axis
  - default reporter:
     - CLI|Markdown table if more than half of cells would be filled, and some TASK.parameters are defined
        - CLI|Markdown list otherwise
     - Markdown table|list if OPTS.insert '*.md|*.markdown|README|readme'
        - CLI table|list otherwise
  - sort BENCHMARK_ARR:
     - sort main() from fastest to slowest (using average of parameters)
     - sort parameters() from fastest to slowest (using average among main() with same name)
  - show standard devation with +|- (to give idea of precision and whether should increase duration)
     - when deviation is 0 (only one `time`), should display differently

Duration reporting:
  - use lowest unit, with minimum 10 (i.e. 2 digits)
  - integers only

[REPORTER_]OPTS.output 'PATH':
  - def: "-" (stdout)

[REPORTER_]OPTS.insert 'PATH':
  - looks for delimiters:
     - "check-speed-start" and "check-speed-end"
        - remove anything in-between
        - require both. Reason: when moving it around the file, user should be aware that both should be moved
     - there can be other things on the line (e.g. <!-- COMMENT --> or #COMMENT), but whole line is kept
     - no parsing needed
  - insert content as is, with no wrapping
  - agnostic to:
     - format of the file to modify
     - format of the inserted content
     - users need to pick ones that make sense, e.g. Markdown reporters for Markdown files

Baseline reporting:
  - OPTS.baseline "PATH" (def: sibling to benchmarked file, e.g. "benchmark.js" -> "benchmark.spyd.json")
     - same content as JSON reporter
        - including all stats
  - OPTS.compare BOOL (def: true):
     - if baseline file exists, compare current results with baseline results and pass comparison to reporters
     - make reporters show difference, e.g. green|red down|up arrow with time difference
     - do not allow compare if OS is different
  - OPTS.save BOOL (def: false):
     - save current results to OPTS.baseline

"Benchmarks created by [check-speed](...)" in reporters:
  - OPTS.credits BOOL (def: true if insert mode)

Reporter showing advanced stats
  - e.g. graphs in terminal

Reporter where the tasks are in both axis, and the cells are the difference in %

Progress reporters:
  - separate from final reporters
  - OPTS.progress "MODULE"_ARR
  - called at regular intervals
  - called with:
     - percentage
     - current task
     - current task stats
  - types:
     - silent (def when OPTS.reporters 'silent')
     - progress bar
     - spinner with task name and current median
     - both above (def)
  - not shown in non-interactive terminal or in CI (see is-interactive package)

OPTS.limit.TASK_ID NUM
  - NUM is max percentage of increase, relative to baseline
     - def: twice standard deviation
  - if above:
     - highlight in reporter
        - no CLI error message
     - exit code 1
  - --limit.TASK_ID on the CLI, but OBJ programatically and in config file

BENCHMARK.system OBJ: hardware and software info

OPTS.platforms STR (use browserlist and PACKAGE.engines)
  - run in different browsers and Node versions
     - i.e. should probably enforce ESM
  - the same logic should be used to launch benchmarks and to load task file itself (to figure out the task names and parameters)
  - create one BENCHMARK per platform, with different BENCHMARK.system
  - abstract to own package
  - should allow adding platforms as plugins

Competitors benchmark:
  - benchmark with other benchmarking tools
  - each should benchmark Math.random() for the same duration
     - use different durations as parameters
  - report both medians (for accuracy) and standard deviation (for precision)

Separate now.js to its own package

Separate resolution.js to its own package

Separate stats to its own package
  - compare with existing packages

Separate measuring part to its own package
  - child processes might be able to spawn only this instead of the full check-speed package

Promote:
  - add keywords (GitHub, package.json)

Features:
  - most precise and accurate benchmarking
  - pretty reporting
  - comparison with previous benchmarks
  - performance testing
  - automatically insert latest benchmarks into your documentation
  - custom reporters
  - TypeScript support

Commercial offer:
  - reporting dashboard:
     - show time series (i.e. keep history)
        - should not lose history when change only the `title` of the function|variant
     - nice data visualization
     - should show function bodies
  - code editor for benchmark files:
     - run benchmarks (inside browser not on our servers)
     - send benchmark results to API (like what users would do on CI otherwise)
     - show results from API
     - i.e. can be used like jsperf
  - Sharing like jsperf:
     - allow users to run in their own browsers
  - PR bot
  - notifications
  - user should report benchmark results to our API
     - like Codecov does
     - i.e. we do not pay for infrastructure cost, except simple CRUD API to store benchmark results
     - should be integrated with CI (i.e. use `ci-info`)
  - pricing:
     - free for open source
     - pay per private repo
