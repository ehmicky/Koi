
        
   SPYD  
        



Learn:
  - divide up chalk
  - inquirer and node-promptly

`remove` command:
  - prompt for removal confirmation
     - after report (i.e. at beginning of `removeFromHistory()`)
     - do not print that confirmation removal succeeded
        - but still print error if failed
  - CONF.force BOOL
     - def: true if isInteractive(process.stdin)
     - alias "f"
     - if false:
        - no report
        - no prompt

Scrolling:
  - only during results preview
     - when either stopping or ending benchmark, report in full without scrolling
  - actions:
     - listen for UpArrow/k and DownArrow/j
     - keep track of current scroll amount
     - remove first and last lines before printing according to scroll amount
     - show top of screen initially, not bottom
  - instructions:
     - shown at bottom of screen, regardless of scroll amount
     - includes progress bar, i.e. is whole progress output
     - show:
         Up/Down: scroll
         CTRL-C: stop
  - not if output fits within process.stdout.rows
     - do not show scrolling instructions either

In isInit(), do not do beforeAll|afterAll()

plugin.config.PROP STR[_ARR]
  - for all plugins: reporter, runner
  - meant for validation, using jest-validate (same validation as for CONF.*):
     - validate against unknown props
        - including automatic suggestion when using wrong case
     - if ARR, use multipleValidOptions()
  - required to use PROP
  - validate PROP matches /^[a-z][a-zA-Z\d]*$/
     - i.e. use - (not _ nor case) as delimiter
  - if STR starts with ./ ../ or /, add to `PATH_CONFIG_PROPS` during config file path resolution
     - including for CONF.runnerId.tasks
        - i.e. CONF.runnerId.tasks in spyd.* is relative to `spyd.yml`'s directory

Categories:
  - combination.categories.CATEGORY "ID"
  - remove current result.categories OBJ_ARR
  - on report (not persisted in stores):
     - filter out unnecessary categories from combination.categories.CATEGORY
        - i.e. any category with 0|1 id
        - exception: if all categories are unnecessary, keep task
        - reason this is not persisted: when merging results, those categories might get additional ids
     - convert combination.categories.CATEGORY "ID" to OBJ:
        - id STR
        - title STR, titlePadded STR: using CONF.titles
           - done on load. CONF.showTitles() just omit those properties later
     - sort result.combinations by mean stats.median
        - do not add combination.*Rank nor categories.*.mean|rank
        - same priority order as result.categories ARR order
        - reason this is not persisted: order might change depending on CONF.include|exclude
     - set result.categories "CATEGORY"_ARR:
        - with all categories
        - sorted by: step, task, runner, system, variation
  - reporters rows|columns computation:
     - use result.categories, including ARR order
     - preserve result.combinations ARR order
     - when grouping categories (e.g. tables), filter out dimensions with no combinations, which can happen due to:
        - CONF.include|exclude
        - runnerConfig variations being runner-specific
  - getCombinationName(combination)->STR:
     - 'CATEGORY "ID",...'
     - ids, not titles
     - for: `exec`, limit errors, measure errors
     - should re-use some of the normalization happening for reporters:
        - result.categories ARR order
        - filter out unnecessary categories
     - not meant for reporters
  - CATEGORY: "task", "step", "runner", "system[.{systemCategory}]", "variation.{variationCategory}"
  - remove current row|column|addTitles() logic

CONF.system "ID" -> CONF.system[.{systemCategory}] "ID"
  - CATEGORY is "system[.{systemCategory}]"
     - i.e. can use several categories
     - def systemCategory: none
     - getCombinationName() should remove "system." prefix from CATEGORY
        - if no systemCategory: "system"
  - system id is "[{systemCategory}.]ID"
     - user-defined id validation should allow ".", but only in the middle, not in systemCategory|ID
  - when sorting result.categories, system sorted:
     - after runner, before variation
     - by CONF.system ARR order
  - result.systems[*].categories OBJ_ARR: id STR, title STR
  - reporters show space-separated system titles
     - i.e. by default, "[{systemCategory}.]ID ..."
  - when merging systems:
     - create shared systems even if only one system.categories matches
        - e.g. same CPU for a specific OS
        - in partially shared systems, system.categories only includes shared ids
     - systems[0] is reserved for shared system where all ids match

Variations:
  - some config properties can be optionally variable:
     - CONF.PROP { ID: VAL, ... } instead of CONF.PROP VAL
     - PROP: "variable property" or "variation category"
     - PROP + ID: variation
  - only on any CONF.* that can change the results:
     - CONF.concurrency
     - CONF.inputs.{inputId}
     - any CONF.runner{runnerId}.PROP
        - cartesian product only to combinations with that runner
     - add comment why not CONF.duration:
        - no reasons why users would want to measure with different CONF.duration
        - complicates implementation
   - CATEGORY is "variation.{variationCategory}"
      - i.e. each variable property is a category
      - variationCategory is property VARR (dot-delimited)
      - getCombinationName() should remove "variation." prefix from CATEGORY
  - when sorting result.categories, variations sorted:
     - at end
     - input, then concurrency, then runnerConfig
     - input: sorted by CONF.inputs OBJ order
     - runnerConfig sorted by:
        - CONF.runner ARR order
        - then config prop name, alphabetically
  - add combination.config CONF_OBJ
     - not persisted in history nor used in report
     - contains config, with combination-specific variations
  - variation id is 'PROP.ID'
     - full property name, including:
        - `runner{RunnerId}.PROP.ID`
        - `inputs.{inputId}.ID`
           - reasons we do not use shorter 'inputId.ID' instead:
              - forward compatiblity with future variable configuration properties
              - consistent/monomorphic, i.e. simpler to learn
     - user-defined id validation should allow ".", but only in the middle, not in ID
  - when merging results:
     - if some PROP uses variations in one result, but not in another
     - then use ID 'default' to convert so all instances of that PROP have a variable property
     - this allows keeping history continuity when introducing variations
  - if several variations have different runner.versions.VAR VAL, they are concatenated as a result.systems[*].versions.VAR STR (commaSpace-separated list)
     - does not mention which variation used which ones. It should be obvious enough from ids or titles
  - when using several runners and runnerConfig variations, the other runners will not have those variations
     - should still set combination.categories.{variationCategory} on those combinations, but with id "", title "", titlePadded " ... "
     - when filtering unnecessary categories, should exclude ones with id ""
     - shoup all be in result.categories "CATEGORY"_ARR
  - result.variations.{variationCategory} VAL
     - persisted in history
        - used the most recent value
     - on report:
        - set combination.categories.{variationCategory}.value VAL
           - should only be reported by reporters able to show details
        - remove result.variations

Context OBJ:
  - initialized to empty OBJ
     - passed to before|after
  - each task initializes its own context OBJ
     - reason: discourage inter-task communication
  - before each iteration, shallow copy that OBJ
     - i.e. each iteration has fresh copy
     - i.e. cannot communicate with next iteration
        - reason: might accidentally get previous iteration state, especially if property is set considitionally
        - other reason: ensure proper garbage collection
        - exception: top-level state, or in properties created during before()
  - passed as named argument
     - reason: bound FUNCs, including arrow FUNCs
  - after each step function (after both repeat+scale loops done), throw if `context` argument reference was changed:
      const oldContext = args.context
      ...
      if (args.context !== oldContext) { ... }
  - advantages over top-level scope (which can still be used)
     - not shared between tasks
     - not shared between iterations
     - does not require declaring a variable
  - add comments about problems with alternatives to single `context` OBJ:
     - separate `context` arguments for input and output (to next step)
        - information meant for later steps must be passed between several steps
     - `context` argument for input, return for output (to next step)
        - information meant for later steps must be passed between several steps
     - `{stepId}` argument for input, return for output
        - custom metrics cannot use `return`, using instead something like `args.measures.push(value)`
        - name conflict with any core argument
           - except inputs, which are validated against duplicates with steps
        - more complex to explain:
           - return vs context
           - `stepId` argument name
           - when repeating a step, only last iteration's return value is used

Steps:
  - export one function per step, i.e. each task value is either:
     - FUNC: same as { main FUNC }
     - OBJ:
        - key is before|after|stepId
        - value is FUNC
  - each function is run serially
     - in the order functions were declarared (runner-specific)
  - steps can communicate to each other using `context`
     - the top-level or global scope can also be used
  - stepId:
     - exported OBJ key
     - runners should enforce "main" as the default stepId
     - validated like other combination user-defined ids: character validation, duplicate ids check
  - processes:
     - at benchmark start, when runner communicates available tasks to parent, it should also return available steps
        - returned as `tasks: { taskId: 'stepId'_ARR, ... }`
     - all steps of a given combination use same process
  - remove beforeEach|afterEach
     - rename beforeAll|afterAll to before|after
     - add comment that runners should avoid specific case for reserved exported names, since users might use different case convention for stepIds
  - each step is a combination category
  - implementation:
     - runner:
        - on start, returns steps to parent
           - ARR in execution order
        - on measure, gets param `steps` OBJ_ARR: id "stepId", scale NUM, repeat NUM
           - ARR in execution order
        - do {
            const context = { ...beforeContext }
            const args = { context }

            for (const { id, scale, repeat } of steps) {
              while (scale--) {
                startTime()
                while (repeat--) {
                  steps[id](args)
                }
                endTime()
              }
            }
          } while (maxLoops--)
        - ensure:
           - last step measured is always real last step, i.e. does not leave state half-finished
           - each step run at least once
        - returns `measures` ARR_ARR_NUM
           - ARR in steps execution order
     - parent:
        - `measureDuration`, `aggregationCountdown` are for whole sample (it is already the case)
        - `maxLoops` = 100ms / sum(steps.map((step) => step.median * step.repeat * step.scale))
        - `combination.steps` OBJ_ARR: id "stepId", ...
           - for all step-wise state: measures, bufferedMeasures, stats, loops, times, repeat, calibrated
           - not for: everything related to minLoopDuration, samples
        - total `benchmarkDuration` does not vary with number of steps
           - fix preview logic (at the moment, it uses combinations.length * CONF.duration)
           - also `measureDuration` measures sum of all steps
           - add comments why:
              - new steps are more likely to be due to splitting existing steps than adding new ones
              - adding steps does not increase `measureDuration`, i.e. decrease preview responsiveness
              - with CONF.include|exclude, all steps are still run.
                It is simpler to explain this by documenting that steps never influence total benchmark duration
  - reporting:
     - report one separate table per step
        - step title should be in top-left corner
     - sorting between tables:
        - by step execution order
        - step groups:
           - right before their earliest child
           - if two step groups have same earliest child, decide using:
              - if latest child is earlier, comes first
              - otherwise, CONF.steps.* index
  - excluding steps with CONF.include|exclude:
     - like any other combination categories:
        - filtered out from the `combinations` array created by `getCombinations()`
        - not persisted in results
        - not reported
     - however, runners always run all steps of a given task, even if excluded
        - providing at least one combination for that task exist
        - i.e. parent process measuring logic ignores steps:
           - at the beginning of measuring logic, combinations with same task but different steps are grouped
           - parent process does not pass any information to runner process about steps, and runner runs them all
           - at the end of measuring logic, combinations are ungrouped to different steps
     - add comments explaining reasons why:
        - we always run all steps:
           - ensure cleanup steps are always run
           - ensure steps never miss data|state created by previous steps
           - users most likely want to restrict reporting, not measuring, when selecting steps with CONF.include|exclude
        - skipping steps is done through CONF.* instead of inside task files contents:
           - allow changing it as CLI flag
        - steps skipping requires user action (setting CONF.*) instead of providing some defaults:
           - encourage users to see steps durations before exclusing them from reporting
           - help users understand how steps can be toggled in/off in case they want to see skipped steps duration
        - we do not skip steps based on some stepId prefix (e.g. _):
           - CONF.include|excluse already provide the feature
           - it would be hard to allow users to explicitly report those steps both exclusively ("only _stepIds") and inclusively ("also _stepIds")
  - step groups:
     - behave like steps except:
        - specified with CONF.steps.stepId 'stepId'_ARR
           - ignored if empty ARR
           - reasons for the syntax:
              - allow non-consecutive steps
              - not verbose (unlike using stepId, e.g. using stepId common prefixes)
        - stats are based on aggregation of other steps stats
           - use other steps stats, not `measures` because the number of `measures` might differ between steps
           - how:
              - samples|minLoopDuration: any
              - median|mean|quantiles|min|max|loops|times: add
              - repeat: Math.round(loops / times)
              - deviation: mean
              - histogram: merge histograms (find solution online)
     - persisted in history
        - as opposed to being dynamically computed during reporting
        - reason: so we can use all `measures` for better stats
     - including|excluding step groups does not have impact on whether its children are included|excluded, and vice-versa
        - reason: users might want to see children only when need details
           - and vice-versa
     - to group all steps, must enumerate the ids of each of them
        - no "*" special token because:
           - it would only make sense if it groups only included steps
           - however:
              - included steps might be change dynamically with CONF.include|exclude
              - and user might not expect that "*" groups different steps then
              - it would also make comparison with previous benchmarks wrong if saved
           - instead, being explicit avoids any confusion
  - add comments about:
     - complex step order:
        - problems:
           - order of steps is static (must always be the same)
           - sub-steps must completly "cover" their parent step
              - e.g. does not allow parallel steps
           - if a step starts after another one, it must end before it
        - solution:
           - user must change the code being measured to allow for a serial mode
           - then add 2 variations, one serial (to measure child steps), one not (to measure parent steps)
     - reasons on why using individual step functions (as opposed to start|end('stepId') utility for example)
        - works with cli runner
        - more declarative, giving more information to core
        - simple interface
        - little room for user misuse, i.e. no need for lots of validation and documentation
        - allow reporting all the steps, including in-between them
        - does not require running the task to know which steps are used
        - does not require setting a default stepId
        - does not require lots of work for the runner
     - measuring logic that's not exposed to users:
        - i.e. different steps within the library implementation
        - should return an EventEmitter and wait for specific events inside each spyd step
     - why before|after are not handled as special kinds of steps:
        - if user wants to measure them, should run them more than once, i.e. use a normal step
        - most users would use it for init|cleanup, i.e. do not want reporting
        - too many differences: only runs once, sets initial context, always at beginning|end, error handling, CONF.precise error handling, etc.

Automatic repeat:
  - `repeat` vs `scale`:
     - both passed to runner.measure()
     - both are per step (not per task)
     - repeat is inside timestamp, scale outside
         while (scale) { start = now(); while (repeat) { stepFunc() }; end = now() }
     - goal:
        - repeat: removing imprecision when step function is faster than resolution or timestamp computation
        - scale: fast steps should be run more often than slow steps because:
           - they are less precise, i.e. each iteration brings more value
           - they take a smaller percentage of the overall CONF.duration
  - `repeat` NUM: keep current logic as is
  - `scale` NUM
     - always passed to runner.measure()
     - value:
        - Math.round(maxStepDuration / currentStepDuration)
           - maxStepDuration = for current task, median duration of slowest step
           - currentStepDuration = median duration of current step
        - i.e. always 1 if single step
  - CONF.precise BOOL
     - def: false
     - if false and task has multiple steps, then:
        - for all steps of that task
        - `repeat` and `scale` are always 1
     - if true and task has multiple steps, then:
        - each step function must be idempotent
           - reason: they will be repeated in repeat|scale loops
        - including: cannot both read+write same property in neither arguments nor top-level scope
           - including:
              - stateful class instances like event emitters and streams
              - measuring any mutating function (e.g. ARR.sort())
        - possible solutions:
           - cloning arguments before mutating them
           - instead of CONF.precise true, increase step function complexity (including increasing input size)
           - split step into its own task
  - report imprecise steps
     - only if CONF.precise false and multiple steps
        - reason: result might be slightly imprecise due to approximation of the repeat algorithm
     - when, if repeat had been used, it would have been >1
     - set combination.imprecise BOOL
        - stats prettify logic prepends ~ to duration
        - only for specific steps with imprecise durations, not whole task
  - add comments:
     - reasons why CONF.precise:
        - does not allow selecting tasks:
           - simpler syntax BOOL
           - prevents comparing steps with very different `repeat` since they would be more|less optimized
        - is opt-in instead of opt-out:
           - adds idempotency constraint gradually, once users have understood first how steps work
           - make the default experience not appear buggy (due to users not understanding the flow)
     - problems with alternative solutions to CONF.scale|precise:
        - common to many of those solutions:
           - since steps share data, they must either have same number of repeats or be idempotent
              - this forbids top-level scope or global changes (e.g. filesystem):
                 - big constraint that might cause many users to make mistakes
           - number of repeats being sub-optimal
           - encourage manual user looping:
              - users should not have to worry about it, and rely on spyd instead
              - based on count instead of duration, which is less precise for faster tasks
              - users are most likely to pick a sub-optimal number of loops
           - require work from user, either in code or to learn utility
        - making user manually loop:
           - either in code or with CONF.repeat.* NUM
        - making CONF.scale the same for all steps of a given task:
           - slower steps would repeat more than needed leading them to:
              - increase task duration, potentially a lot
              - have poorer stats distribution
           - make fast steps run as much as slow steps, leading to poorer precision and inefficient use of total CONF.duration
        - utility to signal start|end of measuring in code:
           - duplicate solution than FUNC steps, which solve a similar problem
        - pass some repeat() utility to task
           - problem: the repeat number would only be known once the task has been run once
        - when deciding which step's optimal repeat number to pick, insteading of using the max, use some value in-between the min and max
           - for example, enforce a max ratio between the min and max
        - enforce the number of repeats does not go over CONF.duration
           - problem: does not work with CONF.duration 0|1
        - enforce the number of repeats does not go over specific duration, e.g. 1s
           - problem: increases sample duration, i.e. reduce responsiveness
           - problem: relies on hardcoded duration, which might not fit all machines' speeds

Manual mode:
  - opt-in
     - ignore all of this unless CONF.[steps.stepId.]manual defined for that step
     - reasons:
        - avoid functions returning value but not intended, e.g. when exported directly
        - avoid returning seconds or ms when ns is expected
  - CONF.[steps.stepId.]manual "UNIT"
     - if no stepId: all steps
     - i.e. same step from different tasks have same unit
        - including if single step for all tasks
  - use hardcoded list of units:
     - list:
        - duration: fs ps ns us|μs ms s m h d
           - i.e. allow custom duration
              - could be useful when task file is measuring another process, e.g. time spent on a server
        - %
        - bytes: B KB|KiB MB|MiB GB|GiB TB|TiB PB|PiB
           - also ...b (bits not bytes)
        - counts: ops
     - enum validation:
        - reasons (as opposed to allow custom counts units):
           - simpler to explain
           - no need for case insensitivity
           - no need to validate max length
     - reasons why no empty string units:
        - ambiguous as user might either intend to use it to specify CONF.unit should not be used, or should be displayed with no units
        - forces distinguishing between different units
  - repeat loop still used
     - because automatic duration still measured, for CONF.rate
     - but do not set combination.imprecise
  - pass `steps[*].manual` true to runner:
     - each measure should then be an ARR of two values:
        - automatic duration NUM
        - step return VAL
  - must return NUM from step function
     - reasons, as opposed to set `measure` argument:
        - argument could be destructured, leading to assignment not working
        - argument would be used for too many things: inputs, message passing, manual measures
        - clear that return value has this type of semantics
     - reason why NUM instead of OBJ: works for every language, including cli runner
  - parent validates NUM:
     - for:
        - all tasks of a given step
        - all measures of a given combination
     - allow:
        - 0
        - floats
     - do not allow:
        - negative floats
        - not numbers
        - NaN and Infinity
        - undefined
  - combination.stats:
     - used for manual measures NUM
     - automatic durations are still:
        - measured (for CONF.rate) in combination.durationStats
        - used for calibration: maxLoops, scale
  - persisted at result.steps.stepId.manual "UNIT"
     - optional, including steps.stepId itself, to keep result small
  - re-use existing unit-specific logic for:
     - automatic scaling
        - e.g. 'ns' -> 's' or 'B' -> 'MB'
     - significant digits|decimals
  - reporting sorting:
     - duration, %: asc
     - bytes, count: desc
     - do not allow configuring|overridding for the moment, to keep things simple, because most users won't need it
  - when merging combinations from different results with same stepId but different unit:
     - if same unit "kind" (duration, %, bytes|bits, count): allow comparing by normalizing stats during mergeResults():
        - find the lowest scale among all units, then multiply to it
        - if mixed manual + auto durations, turn all to manual durations
           - i.e. copy combination.stats to combination.durationStats
           - only if manual unit is duration
        - reason: not losing history when:
           - changing unit scale
           - switching from auto to manual duration
     - if different unit "kind":
        - only keep most recent unit, filtering out previous combinations with different unit kind
        - i.e. units are not a combination category

Rate:
  - CONF.[steps.stepId.]rate BOOL
     - def: false
  - reporting-only
     - not persisted in history
     - reporting flag
  - changes the reported value:
     - duration: 1/medianDuration, i.e. times per duration
     - %, bytes, count: value/medianDuration, i.e. scales the left side
  - sorting order:
     - duration: inverted
     - %, bytes, count: kept
  - reported unit:
     - duration, count: "ops/TIME_UNIT"
     - %, bytes: "UNIT/TIME_UNIT"
  - automatic scaling
     - duration, %, count: focused on TIME_UNIT
     - bytes: focused on UNIT, leaving TIME_UNIT as "s"

CONF.concurrency NUM
  - validate that CONF.concurrency NUM is integer >=1
  - each sample spawns NUM processes in parallel
     - always 1 in `exec` command and during `init`
     - start|end group of processes together
     - use same `params`, including `maxLoops`
     - if one process fails
        - the other ones should continue (for cleanup)
        - but the sample should then propagate error
  - handle spawn errors due to too many processes at once
     - try to remove process limit with ulimit, and see if another error can happen with a high CONF.concurrency, e.g. too many open files
  - add code comments that:
     - CONF.concurrency is meant to measure cost of parallelism
        - both CPU and I/O parallelism
     - if task is I/O bound, it can also improve precision by performing more measures, at the cost of accuracy (due to cost of parallelism)
        - the number where parallel processes start competing for CPU depends on how much duration the task spend on CPU vs I/O
        - above that number:
           - median measure increases much more
           - precision decreases much more
     - move the current code comment from src/measure/combination.js (about spawning processes serially)
     - why different processes instead of Promise.all() in a single process:
        - works for any runner
        - no global scope conflicts
        - uses multiple CPU cores

isAsync:
  - initial check for isAsync:
     - execute func once, without await
     - check if return value is promisable (using p-is-promise)
     - sets func.isAsync BOOL (originally undefined)
     - if isAsync, await return value
  - do the above when func.isAsync undefined && repeat 1
     - add code comment that repeat should always be 1 when func.isAsync undefined, and this probably won't change. It is more of a failsafe.
  - do the above in a `sync_async` dir, next to `sync` and `async` dirs
  - do the above independently for beforeEach, main and afterEach
  - always use await on beforeAll|afterAll, i.e. allow both sync and async
  - remove task.async BOOL

Quantiles|histogram:
  - persist in history
  - some stats should have a space-efficient shape for history, but be denormalized on load:
     - histogram:
        - denormalized: OBJ_ARR: low, high, frequency
        - normalized: ARR of [high, frequency]
     - quantiles:
        - denormalized: OBJ_ARR: percentage NUM, value NUM
        - normalized: NUM_ARR
     - both: use difference from median in durations
  - show in `debug` reporter

Add stats.p95

reporter.debugStats BOOL
  - def: false
  - true for `debug` reporter
  - if false, do not pass:
     - mean
        - add comment that we must ensure median is the main one used, so different reporters are consistent, and also because it is used in sorting combinations, and also it is less precise
     - times
        - add comment that it is a bad indicator of precision, and also might be confused as an indicator of speed due to other benchmark libraries showing it like that
     - minLoopDuration, samples, repeat, loops

Precision:
  - compare precision:
     - compare with multiple processes:
        - difference between combinations of single benchmark, vs between single combinations of different benchmarks
        - median vs standard deviation vs variation between processes
        - small CONF.duration vs big CONF.duration
  - note: using a "for loop" without spyd does work:
     - increasing the count makes the results more and more precise
     - the first 2 loops (regardless of the total count) always seem to be different from others
  - check if using a fixed, low amount of processes helps with precision???
  - find ways to improve precision even more???

History:
  - save results to a `spyd` git branch
     - branch is created from init commit
        - i.e. does not hold reference to any parent commits
     - includes `README.md` explaining the branch
     - switches to `spyd` git branch using git worktree:
        - for both CONF.save and load
        - on load: only if `spyd` branch exists
        - temporary git worktree add + remove
           - using global temp dir
              - filename should be random ID because:
                 - concurrency
                 - prevent re-using previous worktree if not cleaned up
           - use `try {} finally {}` to ensure git worktree remove is called
           - reasons:
              - works even if uncommitted changes
              - faster and less risky than git stash
     - reasons to use git branch:
        - unlike using regular file in codebase:
           - does not pollute git log
           - does not require git push --force on the codebase
           - can update in CI without requiring developers to pull all the time
        - unlike git hash-object + git cat-file:
           - allows multiple files
           - does not create many tags
           - easy to understand
           - no risk of pruning
           - versioned
  - `sync` command:
     - git pull --rebase + git push, on `spyd` git branch
        - git push not run if we know locally there is nothing to push
        - retry (including git pull --rebase) if git push failed due to conflict
           - fails if git fails for any other reasons
           - fails if merge conflict, asking user to fix it
     - stdin|stdout|stderr "inherit" on git pull|push
        - reasons:
           - allow entering passwords
           - show any error message such as: authentication, git hooks, network, etc.
           - provide with progress
        - not on other git commands
        - also prints headers with cyan "Pulling latest results..." and "Pushing new results..."
        - only if CONF.quiet false
     - reasons we separate local (CONF.save) and remote (`sync` command) read|write:
        - mimics git, i.e. easy to understand
        - much faster, since read|write mostly locally
        - easier to isolate, fix and understand many possible failures with git push|pull
  - file structure:
     - individual results:
        - at /benchmark/history/FILE.json
           - FILE is result.id
        - one immutable FILE.json per result
           - i.e. single OBJ
        - reasons (as opposed to single file for all results):
           - fast to create new results
           - does not create git conflicts
           - concurrent safe
           - small file size impact in git history
        - format is JSON
           - reason: fast
     - cache file:
        - at /benchmark/history.json
        - cached concatenation of /benchmark/history/*.json
           - i.e. OBJ_ARR
           - reason: 10 times faster to read single file than individual ones
        - if exists, read instead of /benchmark/history/*.json
        - written at end of commands:
           - not if does not read results (`exec`)
           - if command does not update results (`show`, `remove`, `bench` with CONF.save false, `sync` without git pull): only if `history.json` does not exist yet
           - if command update results (`bench` with CONF.save true, `sync` with git pull): use updated results
        - use atomic-file-write
        - stringify each result separately
           - due to max string size if many results
        - add `.gitignore` in `spyd` branch to ignore it
           - reasons:
              - prevent merge conflicts
              - less size in git repo
  - multi-branch history:
     - sort by:
        - branchIndex in lastResult.systems[0].git.branches
           - filter out if not in any of them
        - then by result.timestamp
           - filter out if result.timestamp > oldest result in child branch
        - should still keep results with same result.systems[0].ci together (as we already do)
     - reasons:
        - easy to understand|explain
        - preserve chronologic order
           - good for reporting
           - good for time-based deltas
        - not impacted by rebasing
        - does not require git, except for finding parent branches
        - mostly follows "git log" order
           - except for newest common commit of parent branch's result, if saved after current branch's first commit's result, but that's unlikely
     - results with no git branch:
        - `previous` always empty ARR
           - including `sincePrevious`
           - i.e. behaves as if part of single-result branch with no parent branch
        - result.systems[0].git.branch empty ARR
  - CONF.since commit|tag|branch:
     - if sinceResult not parent of targetResult, sincePrevious uses a different branch+parentBranches than targetResult's
        - reason: allow comparing branches
           - very useful for PRs, about to be merged|rebased onto
           - still preserving chronological order, except for lastResult
        - if sinceResult child of targetResult: return -1, i.e. empty `previous`
     - logic:
        - branch:
           - use last result in branch+parentBranches
           - reasons:
              - what user probably expects when comparing branches, or to parent branch
              - since branch is explicitly named, try to use it if possible, not child branch
        - commit|tag:
           - get author date (not committer date)
              - using `git`
           - then find first result with result.timestamp >= authorDate, within same branch
              - reasons child branches not used:
                 - comparing between branches is most likely not wanted
                 - if sinceResult not parent, hard to know which child branch is wanted
              - if none found, find first previous result with result.timestamp < authorDate, within branch+parentBranches
                 - reasons:
                    - user might have meant this instead
                    - graceful fallback
     - if none found, returns -1, i.e. empty `previous`
     - reasons we do not use result.systems[0].git:
        - works with rebases (which modify commit hashes) and tag renames
        - more uniform behavior regardless of whether a result with specific commit|tag exists
     - add CONF.since "parent"
        - same as CONF.since "branch" with parent branch
        - if no parent branch, empty `previous`
        - def of CONF.since is still "1"
  - branch aliases
     - reason: do not lose connection between results when git branch renamed
     - result.systems[0].git.branches STR_ARR_ARR:
        - computed on result creation
        - includes all parent branches, in order
           - using `git`
              - previously merged|rebased branches should be ignored, using the branch they merged|rebased onto instead
        - includes all branch renames
           - using `git reflog`
     - on load:
        - find current branches using `git branch`
        - for each branch, add aliases using `git reflog`
        - group results by `git.branches` with at least one common element, and add to branch aliases
        - set normalized result.systems[0].git.branches STR_ARR
           - using the { "ALIAS_BRANCH" -> "CURRENT_BRANCH" } map
  - auto-delete results when git branch has been deleted
     - done on CONF.save
     - done by comparing `git branch` with result.systems[0].git.branches
        - including branch aliases
     - do not delete result with a result.systems[0].git.tag
        - providing the tag still exists (using `git tag`)
     - reasons:
        - performance, by making history.json smaller
        - prevents different branches with same name (created at different times) to be connected
  - require git:
     - only for:
        - CONF.save
        - `sync` command
        - CONF.since branch|commit|tag|"parent"
     - require:
        - `git` binary is executable (i.e. `git --version` has exit code 0)
        - minimum version of `git`
        - there is a `.git` in `[.../]{CONF.cwd}`
     - reasons to store with git:
        - no need to setup any remote store|database
        - much faster (everything local)
        - easy to share results
        - easy to make it work with git branches
        - easier data conflict resolution
        - data is coupled with repository
  - improve performance when history is big
     - potential ideas:
        - removing some object spreads (during destructuring)
        - split `previous` into three arrays, computed at history load-time:
           - `sinceResult`: aggregation of all results from first to CONF.since (included)
           - `allPrevious`: aggregation of all results from sinceResult to lastResult (excluded)
           - `previous`: ARR of each results from sinceResult to lastResult (excluded)
        - add `previous` titles on load

Plugin shape should be validated

Error handling:
  - better way for all plugins (report, runners) to signal user error vs bugs
  - better handling of child process errors due to runner bugs (handled as user error right now)
  - plugin|core errors should print message to report GitHub issues to the plugin|core
     - it should include system information

day.js:
  - parse "timestamp" and "duration" delta format using day.js
  - serialize `result.timestamp` for reporting using day.js

CONF.debug BOOL
  - add debug information, for bug reports
  - add to issue template
  - for all commands
  - print:
     - resolved config
     - task files
     - runner.versions
     - combinations
     - each sample's state (including maxDuration, repeat, etc.)
     - last result, new result
  - do not call reporters

Allow with the node runner:
  - tasks.mjs
  - tasks.ts

Learn package 'simple-statistics' and use it in spyd???

When killing child process, should kill descendants too
  - e.g. with spyd-runner-cli and command 'yes', 'yes' keeps executing after timeout

Consider lowering the valid Node version for spyd-runner-node, so that `run.node.versions` can target lower versions

Create GitHub action

Reporters:
  - types:
     - JSON
     - CLI list
     - CLI table
     - Markdown list
     - Markdown table
     - CLI graphs|histograms
     - CLI where the tasks are in both axis, and the cells are the difference in %
     - CLI with horizontal bars for medians
        - with full CLI width for slowest median
        - still show numbers on top of bars (or on their left side)
        - def reporter instead of simple CLI list, except when there is only one combination
        - for Markdown too???
     - HTML
     - CLI time series (with previous combinations)
  - CLI|Markdown list:
     - follow the formatting I used in fast-cartesian example
        - simple list for TASK with no inputs
  - CLI|Markdown tables:
     - inputs as x axis, tasks as y axis
  - default reporter:
     - CLI|Markdown table if more than half of cells would be filled, and some inputs are defined
        - CLI|Markdown list otherwise
     - Markdown table|list if CONF.output inserts '*.md|*.markdown|README|readme'
        - CLI table|list otherwise

Make `precise-now` work on browser + node

Split `precise-timestamp` to own repository
  - make it work on browser + node
  - problem with browser: performance.now() is made only ms-precise by browser due to security timing attacks

Separate into different repos:
  - some plugins are builtin, i.e. required as production dependencies by core
     - including spyd-run-node and spyd-run-cli (until more runners are created)
  - types: spyd-reporter|runner-*
  - spyd -> spyd (CLI) + spyd-core (non-CLI)

Clarify features in `README`:
  - most precise and accurate benchmarking
  - pretty reporting
  - history
  - performance testing
  - automatically insert latest results into your documentation
  - custom reporters
  - TypeScript support
  - CI-friendly

Add tests, documentation, etc.:
  - for all repos, including sub-repos
  - add keywords (GitHub, package.json)

Utilities to help people creating reporters, runners
  - GitHub template
  - test utility

Competitors benchmark:
  - benchmark with other benchmarking tools
  - each should measure Math.random() for the same duration
     - use different durations as inputs
  - report both medians (for accuracy) and standard deviation (for precision)

Add roadmap:
  - point to it from contribution doc to orient contributors towards features I want (e.g. HTML reporter)

Send PRs to do or redo benchmarks of repositories to
  - get user feedback
  - experience the library as a user
  - get visibility

Promote
  - https://2020.stateofjs.com/en-US/resources/

Ideas for articles about benchmarking:
  - choice between precision and accuracy
  - choice between computing timestamp inside or outside the for-loop, and hybrid approach spyd takes

Add other runners:
  - spyd-runner-chrome (maybe using puppetter)
  - spyd-runner-firefox (maybe using puppetter-firefox)
  - spyd-runner-selenium
  - spyd-runner-bash
  - spyd-runner-go

Commercial offer:
  - reporting dashboard:
     - show time series (i.e. keep history)
        - should not lose history when change only the `title` of the function|variant
     - nice data visualization
     - should show function bodies
  - code editor for tasks files:
     - perform benchmark (inside browser not on our servers)
     - send results to API (like what users would do on CI otherwise)
     - show results from API
     - i.e. can be used like jsperf
  - Sharing like jsperf:
     - allow users to benchmark in their own browsers
  - PR bot
  - notifications
  - user should report results to our API
     - like Codecov does
     - i.e. we do not pay for infrastructure cost, except simple CRUD API to store results
     - should be integrated with CI (i.e. use `ci-info`)
  - pricing:
     - free for open source
     - pay per private repo

==============================================


Default categories:
  - "default" with system so that can keep history???
  - empty with variation???
  - "main" instead of default???
  - is keeping history important??? isn't any new dimension going to lose history???
     - can compare previous combination with subset of dimensions???
     - only a problem for dynamic categories (systems and variations)???
  - maybe use first category in CONF.system|variableProp.*???
  - for steps, use "main"???
     - ask runners to enforce it
        - i.e. return `tasks` `{ taskId: 'stepId'_ARR, ... }`
     - or instead allow returning no steps, and let core default to "main"???
        - i.e. return `tasks` `{ taskId: 'stepId'_ARR, ... }`, but ARR can be empty???
        - pass `steps[*].id` `undefined` in child samples when no steps???

Step execution order:
  - allow two tasks to run steps in different order???
  - if so, how to sort steps as a category???

Step groups:
  - make it reporting-only, instead of measuring-only???

Max loops:
  - MAX_LOOPS 1e7
     - divided by combinations.length (including steps)
  - add `packedMeasures` empty ARR
  - `bufferedLoops`: sum of lengths of all current `bufferedMeasures`
  - `maxLoops` should be Math.min() with MAX_LOOPS - packedMeasures.length - measures.length - bufferedLoops
  - when packedMeasures.length + measures.length + bufferedLoops >= MAX_LOOPS:
     - force aggregation
     - random filter out `packedMeasures` and `measures`
        - after merging `bufferedMeasures` to `measures`
        - do ARR.filter(() => Math.random() < probability)
           - i.e. not precise, but should be ok due to the high length of ARR
           - reason: fast
        - probability of:
           - packedMeasures: 1/packCount
              - packCount initially 1, and incremented before each packing
           - measures 1 - (1/packCount)
        - then mergeSort() `measures` to `packedMeasures`
     - also done at end of benchmark
  - keep stats that can be aggregated to single number???
     - min|max
     - sum (for mean)
  - how does it work with previews???
     - maybe only preview `packedMeasures` once it stopped being empty???
  - does this actually help precision??? If randomly filtering, doesn't the merge have the same precision as the unmerged???
  - alternatives:
     - stop benchmark
     - filter at regular intervals instead of randomly
        - does this change distribution???
        - use quantiles
           - for both arrays, always start with min and end with max, with even spaces between picked items
     - persist `packedMeasures` on-file instead of memory
        - mergeSort() stream file on read+write
        - computeStats() stream file on read
        - previews???

HTTP body max length:
  - 1e8 max HTTP body length (unless streaming used)
  - add `Math.min()` to `maxLoops`???
Try to see if anything more can be removed from `node` runner, i.e. simplified for all runners

Abstract runner orchestration so it is easier to create a runner???
   - how???
      - maybe an execute which maintains a state machine (using combination.id) and returns next action???

Precision stat???
  - means difference between another benchmark with same settings
  - as opposed to standard deviation is variation between measures
  - how to compute???
  - use in progress bar of CONF.duration 0???
     - i.e. would show a combiation as complete once its precision is good enough

Programmatic usage???

Localization:
  - CONF.lang "LANG"
     - e.g. "en_US"
     - def: "en_US", not guessed???
 - changes number formatting
 - translates text in reporters:
    - for common text like "Memory"
    - reporters use a translation utility which uses some shared translations, which can be augmented by contributors (not users)
 - translates titles:
    - allow titles.yml to optionally be OBJ: LANG: OBJ2
    - i.e. defined by users
 - what about units???
    - known units, e.g. duration
    - user-defined units

Slogan???
  - Current: Simple and precise benchmarking
  - Potential issues:
     - "benchmarking" might not be clear enough
     - remove "simple"??? there are simpler alternatives, e.g. just using for loops or console.time()
     - not enough stress on the great DX
        - use word "fun"??? "pretty"???
