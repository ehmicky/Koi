
        
   SPYD  
        



Minimum 3 digits of precision instead of 2:
  - i.e. under 1000s, there are always 3 digits shown: NNN, NN.N or N.NN
  - probably just changing MIN_PRECISION to 1e2 in decimals.js might be enough
  - units.js probably can remain the same: use smallest unit where value >= 1

Remove days/hours/minutes units:
  - reasons (note them):
     - hours-long tasks are unlikely
     - minutes require writing two units (e.g. 1m56s) which is harder to read
     - minutes are not base 10, which makes it harder to compare

now.js should not export `name`

Try different minTime and see impact on stats

Check src/stats/print/ and add comments

BENCHMARK.iterations[*].name: should pad task title and parameter separately

eslint-config-standard-prettier-fp: add 10 as an exception to no-magic-numbers

Standard deviation:
  - in printedStats, should prepend ± (before left padding)
  - reporters should only show it when OPTS.verbose true (def: false)
      - should be wrapped in parenthesis

"Benchmarks created by [spyd](...)" in reporters:
  - OPTS.credits BOOL (def: true if insert mode)

BENCHMARK.system OBJ:
  - only information that impacts benchmarks:
     - hardware
     - OS
     - runtime (node)
  - OPTS.system BOOL:
     - ask reporters to print it
     - def: false
  - do not merge with OPTS.platform

OPTS.data 'DIR':
  - def: "./spyd/" sibling to task file
  - save file to DATA_DIR/TASK_FILE.ID.TIMESTAMP.json:
     - TASK_FILE: filename without extension
     - ID:
        - for each unique BENCHMARK.system
        - incremental integer
     - TIMESTAMP: UTC human friendly in secs
  - same content as JSON reporter
     - including all stats
  - can be HTTP[S] URL:
     - do GET ?file=TASK_FILE&platform=SYSTEM_JSON
         - retrieve all matching BENCHMARK_ARR
     - do POST BENCHMARK
        - on --save
  - BENCHMARK.iterations[*].diff NUM: percentage of change of median since latest data (if any)
     - not persisted to data file, but passed to reporters
  - BENCHMARK.iterations[*].previous BENCHMARK_ARR
     - not persisted to data file, but passed to reporters

OPTS.save BOOL (def: false):
  - save current results to data file

OPTS.compare BOOL (def: true):
  - make reporters show BENCHMARK.iterations[*].diff, e.g. green|red down|up arrow with time difference

OPTS.limit.TASK_ID NUM
  - NUM is max percentage of increase, relative to data file
     - def: twice standard deviation
  - if above:
     - highlight in reporter
        - no CLI error message
     - exit code 1
  - --limit.TASK_ID on the CLI, but OBJ programatically and in config file

Add progress reporters:
  - spinner with task name and current median
  - progress bar
  - both above (def)

Reporters:
  - types:
     - JSON
     - CLI list
     - CLI table
     - Markdown list
     - Markdown table
     - CLI graphs|histograms
     - CLI where the tasks are in both axis, and the cells are the difference in %
     - CLI with horizontal bars for medians
        - with full CLI width for slowest median
        - still show numbers on top of bars (or on their left side)
        - def reporter instead of simple CLI list, except when there is only one iteration
        - for Markdown too???
     - HTML
     - CLI reporter that shows BENCHMARK.iterations[*].previous as a time series
  - CLI|Markdown list:
     - follow the formatting I used in fast-cartesian example
        - simple list for TASK with no TASK.parameters
  - CLI|Markdown tables:
     - parameters as x axis, mains as y axis
  - default reporter:
     - CLI|Markdown table if more than half of cells would be filled, and some TASK.parameters are defined
        - CLI|Markdown list otherwise
     - Markdown table|list if OPTS.insert '*.md|*.markdown|README|readme'
        - CLI table|list otherwise

BENCHMARK.ci OBJ:
  - CI information using library like ci-info
  - used e.g. by HTML reporter

BENCHMARK.git OBJ:
  - git information (commit, branch, etc.)
  - only if `git` installed
  - used e.g. by HTML reporter

OPTS.platforms STR (use browserlist and PACKAGE.engines)
  - run in different browsers and Node versions
     - i.e. should probably enforce ESM
  - the same logic should be used to launch benchmarks and to load task file itself (to figure out the task names and parameters)
  - create one BENCHMARK per platform
  - abstract to own package
  - should allow adding platforms as plugins

Competitors benchmark:
  - benchmark with other benchmarking tools
  - each should benchmark Math.random() for the same duration
     - use different durations as parameters
  - report both medians (for accuracy) and standard deviation (for precision)

Separate each reporter to own package
  - builtin reporters are required as production dependencies by core

Separate now.js to its own package

Separate resolution.js to its own package

Separate stats to its own package
  - compare with existing packages

Separate measuring part to its own package
  - child processes might be able to spawn only this instead of the full spyd package

Programming language agnosticism???
  - spyd-*:
     - runs tasks and produces JSON stream to stdout
        - instead of current IPC events, but same content otherwise
        - end of stream is end of tasks
     - for each programming language (e.g. spyd-js)
  - opts.require: generalize it to any CLI options passed to the binary (e.g. node)???
  - BENCHMARK.system: runtime (e.g. node version) is language specific???

Add roadmap:
  - point to it from contribution doc to orient contributors towards features I want (e.g. HTML reporter)

Promote:
  - add keywords (GitHub, package.json)

Features:
  - most precise and accurate benchmarking
  - pretty reporting
  - comparison with previous benchmarks
  - performance testing
  - automatically insert latest benchmarks into your documentation
  - custom reporters
  - TypeScript support

Commercial offer:
  - reporting dashboard:
     - show time series (i.e. keep history)
        - should not lose history when change only the `title` of the function|variant
     - nice data visualization
     - should show function bodies
  - code editor for benchmark files:
     - run benchmarks (inside browser not on our servers)
     - send benchmark results to API (like what users would do on CI otherwise)
     - show results from API
     - i.e. can be used like jsperf
  - Sharing like jsperf:
     - allow users to run in their own browsers
  - PR bot
  - notifications
  - user should report benchmark results to our API
     - like Codecov does
     - i.e. we do not pay for infrastructure cost, except simple CRUD API to store benchmark results
     - should be integrated with CI (i.e. use `ci-info`)
  - pricing:
     - free for open source
     - pay per private repo
