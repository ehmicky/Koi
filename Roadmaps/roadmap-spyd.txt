
        
   SPYD  
        



Do a small release

Learn ky-universal and got and use in all my libraries
  - use cache with spyd

spyd.json -> spyd.yml

FILE.version NUM
  - saved data file major version
     - different from spyd PACKAGE.version
  - on store.load(), if old FILE.version, throw error saying "spyd migrate" should be run
  - add "spyd migrate" action that does the migration:
     - add code for each new major release
     - requires store.replace(BENCHMARK_ARR, STORE_OPTS)
  - for reported|returned BENCHMARK (as opposed to saved data), use normal PACKAGE.version major release instead

Go through whole code once last time (except CLI and Node runners, already done)

Do release

Competitors benchmark:
  - benchmark with other benchmarking tools
  - each should benchmark Math.random() for the same duration
     - use different durations as variations
  - report both medians (for accuracy) and standard deviation (for precision)

When killing child process, should kill descendants too
  - e.g. with spyd-runner-cli and command 'yes', 'yes' keeps running after timeout

Add progress reporters:
  - spinner with task name and current median
  - progress bar
  - both above (def)

Reporters:
  - types:
     - JSON
     - CLI list
     - CLI table
     - Markdown list
     - Markdown table
     - CLI graphs|histograms
     - CLI where the tasks are in both axis, and the cells are the difference in %
     - CLI with horizontal bars for medians
        - with full CLI width for slowest median
        - still show numbers on top of bars (or on their left side)
        - def reporter instead of simple CLI list, except when there is only one iteration
        - for Markdown too???
     - HTML
     - CLI reporter that shows BENCHMARK.iterations[*].previous as a time series
  - CLI|Markdown list:
     - follow the formatting I used in fast-cartesian example
        - simple list for TASK with no variations
  - CLI|Markdown tables:
     - variations as x axis, tasks as y axis
  - default reporter:
     - CLI|Markdown table if more than half of cells would be filled, and some TASK.variations are defined
        - CLI|Markdown list otherwise
     - Markdown table|list if OPTS.insert '*.md|*.markdown|README|readme'
        - CLI table|list otherwise
  - should use iteration.fastest BOOL for highlighting
  - when BENCHMARK.histogram|percentiles [] reporters should show "Not enough data" (but not throw)
  - Markdown|HTML reporters should add "Benchmarked with [spyd](https://github.com/ehmicky/spyd)" when REPORTER_OPTS.link true
     - but CLI reporters should never add anything (except 'debug' reporter)

Separate each reporter to own package
  - e.g. spyd-report-*
  - builtin reporters are required as production dependencies by core

Separate now.js to its own package

Separate resolution.js to its own package

Separate stats to its own package
  - compare with existing packages

Separate measuring part to its own package
  - child processes might be able to spawn only this instead of the full spyd package

Separate runners into own packages
  - e.g. spy-run-node
  - distributed with npm, JavaScript endpoint, but can otherwise use any programming language

Add other runners:
  - spyd-run-chrome (maybe using puppetter)
  - spyd-run-firefox (maybe using puppetter-firefox)
  - spyd-run-selenium
  - spyd-run-bash

Validation|testing utility to help people creating reporters, runners, progress reporters

Consider lowering the valid Node version for spyd-runner-node, so that `run.node.versions` can target lower versions

Add roadmap:
  - point to it from contribution doc to orient contributors towards features I want (e.g. HTML reporter)

Promote:
  - add keywords (GitHub, package.json)

Features:
  - most precise and accurate benchmarking
  - pretty reporting
  - comparison with previous benchmarks
  - performance testing
  - automatically insert latest benchmarks into your documentation
  - custom reporters
  - TypeScript support

Commercial offer:
  - reporting dashboard:
     - show time series (i.e. keep history)
        - should not lose history when change only the `title` of the function|variant
     - nice data visualization
     - should show function bodies
  - code editor for benchmark files:
     - run benchmarks (inside browser not on our servers)
     - send benchmark results to API (like what users would do on CI otherwise)
     - show results from API
     - i.e. can be used like jsperf
  - Sharing like jsperf:
     - allow users to run in their own browsers
  - PR bot
  - notifications
  - user should report benchmark results to our API
     - like Codecov does
     - i.e. we do not pay for infrastructure cost, except simple CRUD API to store benchmark results
     - should be integrated with CI (i.e. use `ci-info`)
  - pricing:
     - free for open source
     - pay per private repo
