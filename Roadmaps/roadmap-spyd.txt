
        
   SPYD  
        



some eslint fails: autoserver, test-api, spyd, nve. Try gpp

npm i -g nve (for 2.1.0)

Upgrade all my .travis.yml to use `dist: bionic`

nve (release/v3 branch):
  - figure out why commented test in test/cli.js fails with nyc
  - options parsing:
     - use yargs
     - make sure positional arguments are not altered at all
     - pass as OPTS.* in nve("VERSION", "COMMAND", ["ARG"_ARR][, OPTS])
  - ENVVAR NVE_PROGRESS -> OPTS.progress BOOL
  - OPTS.onCreate(CHILD_PROCESS, "VERSION", INDEX)[->PROMISE]:
     - "VERSION" is input, not normalized
     - CLI should use it to pipe output
     - CLI should return PROMISE awaiting child process stdout|stderr
     - should await all PROMISEs at end, once every CHILD_PROCESS has been created???
        - what if serial run, i.e. PROMISE rejected earlier???
  - return value should be PROMISE resolved with ARR of { code, signal } or rejected with ERROR when whole run is done
  - Allow running multiple Node versions at once:
     - space-separated VERSION
     - versions order: same as input for both execution and output
     - execution is in parallel
        - unless OPTS.serial true (def: false)
     - output:
        - each output printed as is
        - if parallel, first child process output is piped/streamed, next ones are buffered and printed whole
        - if serial, every child process output is piped/streamed
        - if more than one version, each version output is:
           - prepended with a line showing the "Node.js VERSION"
              - VERSION is same as input, not normalized
              - in green unless OPTS.colors false (def: if stdout supports colors)
          - appended with an empty line
     - OPTS.wait BOOL:
        - false (def): stop on first exit code != 0 and use it
        - true:
           - wait for all child processes
  - check if `xz --version` exit code is 0, and if so, download *.tar.xz instead of *.tar.gz
     - except for AIX or if Node <0.12.10
  - clean cache:
     - max cache size 10
     - remove last used first
        - not last created
        - which of atime|mtime|ctime is most reliable and relevant???
     - clean when nve called
  - go through whole nve code again
  - update benchmarks
  - fix documentation and examples
     - include the fact that since this is now a full-fledge alternative to nvm, no need to point to it anymore
  - submit nve to https://github.com/sindresorhus/awesome-nodejs
  - spyd --run.node.versions should use nve somehow (instead of get-node), so that child processes use the right Node.js version

Learn package 'simple-statistics' and use it in spyd

Separate into different repos:
  (builtin plugins are required as production dependencies by core)
  - spyd-report-*
  - spyd-run-*
  - spyd-store-*
  - spyd-progress-*

Add progress reporters:
  - spinner with task name and current median
  - progress bar
  - both above (def)

Reporters:
  - types:
     - JSON
     - CLI list
     - CLI table
     - Markdown list
     - Markdown table
     - CLI graphs|histograms
     - CLI where the tasks are in both axis, and the cells are the difference in %
     - CLI with horizontal bars for medians
        - with full CLI width for slowest median
        - still show numbers on top of bars (or on their left side)
        - def reporter instead of simple CLI list, except when there is only one iteration
        - for Markdown too???
     - HTML
     - CLI time series (with previous iterations)
  - CLI|Markdown list:
     - follow the formatting I used in fast-cartesian example
        - simple list for TASK with no variations
  - CLI|Markdown tables:
     - variations as x axis, tasks as y axis
  - default reporter:
     - CLI|Markdown table if more than half of cells would be filled, and some TASK.variations are defined
        - CLI|Markdown list otherwise
     - Markdown table|list if OPTS.insert '*.md|*.markdown|README|readme'
        - CLI table|list otherwise
  - when BENCHMARK.histogram|percentiles [], reporters should show "Not enough data" (but not throw)
  - Markdown|HTML reporters should add "Benchmarked with [spyd](https://github.com/ehmicky/spyd)" when REPORTER_OPTS.link true
     - but CLI reporters should never add anything (except 'debug' reporter)

When killing child process, should kill descendants too
  - e.g. with spyd-runner-cli and command 'yes', 'yes' keeps running after timeout

Competitors benchmark:
  - benchmark with other benchmarking tools
  - each should benchmark Math.random() for the same duration
     - use different durations as variations
  - report both medians (for accuracy) and standard deviation (for precision)

Add tests, documentation, etc.:
  - for all repos, including sub-repos

Consider lowering the valid Node version for spyd-runner-node, so that `run.node.versions` can target lower versions

Validation|testing utility to help people creating reporters, runners, progress reporters

Add other runners:
  - spyd-run-chrome (maybe using puppetter)
  - spyd-run-firefox (maybe using puppetter-firefox)
  - spyd-run-selenium
  - spyd-run-bash

Add roadmap:
  - point to it from contribution doc to orient contributors towards features I want (e.g. HTML reporter)

Promote:
  - add keywords (GitHub, package.json)

Features:
  - most precise and accurate benchmarking
  - pretty reporting
  - comparison with previous benchmarks
  - performance testing
  - automatically insert latest benchmarks into your documentation
  - custom reporters
  - TypeScript support
  - CI-friendly

Commercial offer:
  - reporting dashboard:
     - show time series (i.e. keep history)
        - should not lose history when change only the `title` of the function|variant
     - nice data visualization
     - should show function bodies
  - code editor for benchmark files:
     - run benchmarks (inside browser not on our servers)
     - send benchmark results to API (like what users would do on CI otherwise)
     - show results from API
     - i.e. can be used like jsperf
  - Sharing like jsperf:
     - allow users to run in their own browsers
  - PR bot
  - notifications
  - user should report benchmark results to our API
     - like Codecov does
     - i.e. we do not pay for infrastructure cost, except simple CRUD API to store benchmark results
     - should be integrated with CI (i.e. use `ci-info`)
  - pricing:
     - free for open source
     - pay per private repo
