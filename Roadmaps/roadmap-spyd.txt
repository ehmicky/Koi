
        
   SPYD  
        


------------------------------- TEMPORARY


Manual mode:
  - opt-in
     - ignore all of this unless CONF.[steps.stepId.]manual defined for that step
     - reasons:
        - avoid functions returning value but not intended, e.g. when exported directly
        - avoid returning seconds or ms when ns is expected
  - CONF.[steps.stepId.]manual "UNIT"
     - if no stepId: all steps
     - i.e. same step from different tasks have same unit
        - including if single step for all tasks
     - validation:
        - max length 4
           - i.e. must be abbreviations
           - omit trailing dots
        - not empty string
           - reason: ambiguous as user might either intend to use it to specify CONF.unit should not be used, or should be displayed with no units
           - reason: forces distinguishing between different units
  - use hardcoded list of units:
     - list:
        - duration: fs ps ns us|μs ms s m h d
           - i.e. allow custom duration
              - could be useful when task file is measuring another process, e.g. time spent on a server
        - %
        - bytes: B KB MB GB TB PB
           - also KiB ...
           - also ...b (bits not bytes)
        - counts: x req err
           - also used as the default fallback
     - match unit case-insensitively, but report using same case as specified by others
  - repeat always 1
     - do not set combination.imprecise
  - pass `steps[*].manual` true to runner:
     - each measure should then be an ARR of two values:
        - automatic duration NUM
        - step return VAL
  - must return NUM from step function
     - reasons, as opposed to set `measure` argument:
        - argument could be destructured, leading to assignment not working
        - argument would be used for too many things: inputs, message passing, manual measures
        - clear that return value has this type of semantics
     - reason why NUM instead of OBJ: works for every language, including cli runner
  - parent validates NUM:
     - for:
        - all tasks of a given step
        - all measures of a given combination
     - allow:
        - 0
        - floats
     - do not allow:
        - negative floats
        - not numbers
        - NaN and Infinity
        - undefined
  - combination.stats:
     - used for manual measures NUM
     - automatic durations are still:
        - measured (for CONF.rate) in combination.durationStats
        - used for calibration: maxLoops, scale
  - persisted at result.steps.stepId.manual "UNIT"
     - optional, including steps.stepId itself, to keep result small
     - on load, normalized to results.steps OBJ_ARR: id, manual
  - re-use existing unit-specific logic for:
     - automatic scaling
        - e.g. 'ns' -> 's' or 'B' -> 'MB'
     - significant digits|decimals
  - reporting sorting:
     - duration, %: asc
     - bytes, count: desc
     - do not allow configuring|overridding for the moment, to keep things simple, because most users won't need it
  - what if different results compared for the same step but changed:
     - the "UNIT"???
     - whether auto|manual is used???
  - should it be a reporting-only option???

Rate:
  - CONF.[steps.stepId.]rate BOOL
     - def: false
  - reporting-only
     - not persisted in stores
     - reporting flag
  - changes the reported value:
     - duration: 1/medianDuration, i.e. times per duration
     - %, bytes, count: value/medianDuration, i.e. scales the left side
  - sorting order:
     - duration: inverted
     - %, bytes, count: kept
  - reported unit:
     - duration, count: "x/TIME_UNIT"
     - %, bytes: "UNIT/TIME_UNIT"
  - automatic scaling
     - duration, %, count: focused on TIME_UNIT
     - bytes: focused on UNIT, leaving TIME_UNIT as "s"

`context` type:
  - if plain OBJ:
     - what if user changes the type???
     - make sure args.context is accessed lazily in case user re-assigns property
  - if any type:
     - makes shallow merge harder (to get copy of `before` context at the beginning of each iteration)
        - not possible if type is a class instance???
     - harder to add specific properties for it in the future

Make before|after special steps???
  - except:
     - only run once
     - before sets initial context OBJ for other steps
     - run at beginning|end
     - not taken into account for precise mode
  - i.e. measured, reported, selectable

Error handling:
  - propagate afterAll() errors even if was called due to another exception
     - reason: afterAll() should take into account that it might be called on an exception, and still do proper cleanup
  - do not call afterAll() if an exception happened in beforeAll()

Context OBJ:
  - initialized to empty OBJ
     - passed to before|after
  - before each iteration, shallow copy that OBJ
     - i.e. each iteration has fresh copy
     - i.e. cannot communicate with next iteration
        - reason: might accidentally get previous iteration state, especially if property is set considitionally
        - other reason: ensure proper garbage collection
        - exception: top-level state, or in properties created during before()
  - passed as named argument
     - reason: bound FUNCs, including arrow FUNCs
  - advantages over top-level scope (which can still be used)
     - not shared between tasks
     - not shared between iterations
     - does not require declaring a variable
  - add comments about problems with alternatives to single `context` OBJ:
     - separate `context` arguments for input and output (to next step)
        - information meant for later steps must be passed between several steps
     - `context` argument for input, return for output (to next step)
        - information meant for later steps must be passed between several steps
     - `{stepId}` argument for input, return for output
        - custom metrics cannot use `return`, using instead something like `args.measures.push(value)`
        - name conflict with any core argument
           - except inputs, which are validated against duplicates with steps
        - more complex to explain:
           - return vs context
           - `stepId` argument name
           - when repeating a step, only last iteration's return value is used

Multiple tasks per file:
  - reasons:
     - user-friendlier if small tasks and/or single steps
     - prevents users declaring steps when meant tasks
  - each exported variable:
     - key is taskId
        - taskId is not specified in filename anymore
        - runners should enforce that no duplicate taskId is exported
           - making users use an OBJ enforces this automatically
           - using same taskId with different runners is allowed though
           - keep check for duplicate of runnerId + taskId: but make it a PluginError
     - value is either:
        - FUNC: same as { main FUNC }
        - OBJ:
           - key is beforeAll|beforeEach|afterAll|afterEach|main
           - value is FUNC
     - reason why not concatenating taskId and stepId names: harder for user to distinguish them
  - each task initializes its own context OBJ
     - reason: discourage inter-task communication
  - processes:
     - each combination should have its own process, even if tasks file has multiple tasks declared
        - reason: avoid tasks to influence each other with:
           - global state
           - engine optimization
     - should pass `task: 'taskId'` as spawnArgs:
        - at benchmark start, pass undefined:
           - return `tasks: 'taskId'_ARR` to parent
           - exit right away
        - for each combination, pass taskId:
           - do not return any `tasks` to parent
           - use only this task for measuring

Task selection:
  - no CONF.tasks.RUNNER
  - CONF.runner STR[_ARR]
     - def: 'node'
        - reason not external ones: cannot know for sure whether installed or not
        - reason not ['node', 'cli']: enforce that every CONF.runner has a task file, instead of optionally having one
     - cannot be empty ARR
  - CONF.tasks "PATH"
     - like CONF.runnerId.tasks "PATH"
     - for all runners, except the ones which have CONF.runnerId.tasks defined
     - def: undefined
  - CONF.runnerId.tasks "PATH"
     - only checked if part of CONF.runners
     - error if file does not exist
        - including when using the default value
     - default:
        - tasks.EXT
           - EXT is any from runner.extensions "EXT"_ARR exported by runner
        - cwd is normal cwd, i.e. spyd.yml directory
     - do not allow several tasks files per runner (using ARR or globbing):
        - reason: user might not understand that only entry files must be specified, creating confusing errors
        - no need for junk library anymore

Automatic repeat:
  - `repeat` vs `scale`:
     - both passed to runner.measure()
     - both are per step (not per task)
     - repeat is inside timestamp, scale outside
         while (scale) { start = now(); while (repeat) { stepFunc() }; end = now() }
     - goal:
        - repeat: removing imprecision when step function is faster than resolution or timestamp computation
        - scale: fast steps should be run more often than slow steps because:
           - they are less precise, i.e. each iteration brings more value
           - they take a smaller percentage of the overall CONF.duration
  - `repeat` NUM: keep current logic as is
  - `scale` NUM
     - always passed to runner.measure()
     - value:
        - Math.round(maxStepDuration / currentStepDuration)
           - maxStepDuration = for current task, median duration of slowest step
           - currentStepDuration = median duration of current step
        - i.e. always 1 if single step
  - CONF.precise BOOL
     - def: false
     - if false and task has multiple steps, then:
        - for all steps of that task
        - `repeat` and `scale` are always 1
     - if true and task has multiple steps, then:
        - each step function must be idempotent
           - reason: they will be repeated in repeat|scale loops
        - including: cannot both read+write same property in neither arguments nor top-level scope
           - including:
              - stateful class instances like event emitters and streams
              - measuring any mutating function (e.g. ARR.sort())
        - possible solutions:
           - cloning arguments before mutating them
           - instead of CONF.precise true, increase step function complexity (including increasing input size)
           - split step into its own task
  - report imprecise steps
     - only if CONF.precise false and multiple steps
        - reason: result might be slightly imprecise due to approximation of the repeat algorithm
     - when, if repeat had been used, it would have been >1
     - set combination.imprecise BOOL
        - stats prettify logic prepends ~ to duration
        - only for specific steps with imprecise durations, not whole task
  - add comments:
     - reasons why CONF.precise:
        - does not allow selecting tasks:
           - simpler syntax BOOL
           - prevents comparing steps with very different `repeat` since they would be more|less optimized
        - is opt-in instead of opt-out:
           - adds idempotency constraint gradually, once users have understood first how steps work
           - make the default experience not appear buggy (due to users not understanding the flow)
     - problems with alternative solutions to CONF.scale|precise:
        - common to many of those solutions:
           - since steps share data, they must either have same number of repeats or be idempotent
              - this forbids top-level scope or global changes (e.g. filesystem):
                 - big constraint that might cause many users to make mistakes
           - number of repeats being sub-optimal
           - encourage manual user looping:
              - users should not have to worry about it, and rely on spyd instead
              - based on count instead of duration, which is less precise for faster tasks
              - users are most likely to pick a sub-optimal number of loops
           - require work from user, either in code or to learn utility
        - making user manually loop:
           - either in code or with CONF.repeat.* NUM
        - making CONF.scale the same for all steps of a given task:
           - slower steps would repeat more than needed leading them to:
              - increase task duration, potentially a lot
              - have poorer stats distribution
           - make fast steps run as much as slow steps, leading to poorer precision and inefficient use of total CONF.duration
        - utility to signal start|end of measuring in code:
           - duplicate solution than FUNC steps, which solve a similar problem
        - pass some repeat() utility to task
           - problem: the repeat number would only be known once the task has been run once
        - when deciding which step's optimal repeat number to pick, insteading of using the max, use some value in-between the min and max
           - for example, enforce a max ratio between the min and max
        - enforce the number of repeats does not go over CONF.duration
           - problem: does not work with CONF.duration 0|1
        - enforce the number of repeats does not go over specific duration, e.g. 1s
           - problem: increases sample duration, i.e. reduce responsiveness
           - problem: relies on hardcoded duration, which might not fit all machines' speeds

Step functions:
  - export one function per step, i.e. each task value is either:
     - FUNC: same as { main FUNC }
     - OBJ:
        - key is before|after|stepId
        - value is FUNC
  - each function is run serially
     - in the order functions were declarared (runner-specific)
  - steps can communicate to each other using `context`
     - the top-level or global scope can also be used
  - identifier:
     - stepId, i.e. exported OBJ key
     - runners should enforce "main" as the default stepId
     - validated like other combination user-defined ids: character validation, duplicate ids check
        - including CONF.steps.stepId
  - processes:
     - at benchmark start, when runner communicates available tasks to parent, it should also return available steps
        - returned as `tasks: { taskId: 'stepId'_ARR, ... }`
     - all steps of a given combination use same process
  - requires spawning processes earlier, in order to know available steps
     - also to know whether repeat should be used
  - remove beforeEach|afterEach
     - remove error handling tied to afterEach
     - rename beforeAll|afterAll to before|after
     - add comment that runners should avoid specific case for reserved exported names, since users might use different case convention for stepIds
  - each step is a combination category:
     - selection in CONF.include|exclude|limit|titles
     - part of CONF.rowName, not CONF.columnName
     - result.steps OBJ_ARR
     - combination.steps ARR
  - implementation:
     - runner:
        - on start, returns `steps` "stepId"_ARR to parent
           - ARR in execution order
        - on measure, gets param `steps` OBJ_ARR: id "stepId", scale NUM, repeat NUM
           - ARR in execution order
        - do {
            const context = { ...beforeContext }
            const args = { context }

            for (const { id, scale, repeat } of steps) {
              while (scale--) {
                startTime()
                while (repeat--) {
                  steps[id](args)
                }
                endTime()
              }
            }
          } while (maxLoops--)
        - ensure:
           - last step measured is always real last step, i.e. does not leave state half-finished
           - each step run at least once
        - returns `measures` ARR_ARR_NUM
           - ARR in steps execution order
     - parent:
        - `measureDuration`, `aggregationCountdown` are for whole sample (it is already the case)
        - `maxLoops` = 100ms / sum(steps.map((step) => step.median * step.repeat * step.scale))
        - `combination.steps` OBJ_ARR: id "stepId", ...
           - for all step-wise state: measures, bufferedMeasures, stats, loops, times, repeat, calibrated
           - not for: everything related to minLoopDuration, samples
  - reporting:
     - report one separate table per step
        - step title should be in top-left corner
     - sorting between tables:
        - by step execution order
        - step groups:
           - right before their earliest child
           - if two step groups have same earliest child, decide using:
              - if latest child is earlier, comes first
              - otherwise, CONF.steps.* index
  - excluding steps with CONF.include|exclude:
     - like any other combination categories:
        - filtered out from the `combinations` array created by `getCombinations()`
        - not persisted in results
        - not reported
     - however, runners always run all steps of a given task, even if excluded
        - providing at least one combination for that task exist
        - i.e. parent process measuring logic ignores steps:
           - at the beginning of measuring logic, combinations with same task but different steps are grouped
           - parent process does not pass any information to runner process about steps, and runner runs them all
           - at the end of measuring logic, combinations are ungrouped to different steps
     - add comments explaining reasons why:
        - we always run all steps:
           - ensure cleanup steps are always run
           - ensure steps never miss data|state created by previous steps
           - users most likely want to restrict reporting, not measuring, when selecting steps with CONF.include|exclude
        - skipping steps is done through CONF.* instead of inside task files contents:
           - allow changing it as CLI flag
        - steps skipping requires user action (setting CONF.*) instead of providing some defaults:
           - encourage users to see steps durations before exclusing them from reporting
           - help users understand how steps can be toggled in/off in case they want to see skipped steps duration
        - we do not skip steps based on some stepId prefix (e.g. _):
           - CONF.include|excluse already provide the feature
           - it would be hard to allow users to explicitly report those steps both exclusively ("only _stepIds") and inclusively ("also _stepIds")
  - step groups:
     - behave like steps except:
        - specified with CONF.steps.stepId 'stepId'_ARR
           - ignored if empty ARR
           - reasons for the syntax:
              - allow non-consecutive steps
              - not verbose (unlike using stepId, e.g. using stepId common prefixes)
        - stats are based on a sum of the `measures` of their child steps
           - i.e. we must ensure the number of `measures` for a given tasks is same for each of its steps
     - computed after each sample
        - then persisted in stores
           - as opposed to being dynamically computed during reporting
        - reason: so we can use all `measures` for better stats
     - including|excluding step groups does not have impact on whether its children are included|excluded, and vice-versa
        - reason: users might want to see children only when need details
           - and vice-versa
     - to group all steps, must enumerate the ids of each of them
        - no "*" special token because:
           - it would only make sense if it groups only included steps
           - however:
              - included steps might be change dynamically with CONF.include|exclude
              - and user might not expect that "*" groups different steps then
              - it would also make comparison with previous benchmarks wrong if saved
           - instead, being explicit avoids any confusion
  - add comments about:
     - complex step order:
        - problems:
           - order of steps is static (must always be the same)
           - sub-steps must completly "cover" their parent step
              - e.g. does not allow parallel steps
           - if a step starts after another one, it must end before it
        - solution:
           - user must change the code being measured to allow for a serial mode
           - then add 2 variations, one serial (to measure child steps), one not (to measure parent steps)
     - reasons on why using individual step functions (as opposed to start|end('stepId') utility for example)
        - works with cli runner
        - more declarative, giving more information to core
        - simple interface
        - little room for user misuse, i.e. no need for lots of validation and documentation
        - allow reporting all the steps, including in-between them
        - does not require running the task to know which steps are used
        - does not require setting a default stepId
        - does not require lots of work for the runner
     - measuring logic that's not exposed to users:
        - i.e. different steps within the library implementation
        - should return an EventEmitter and wait for specific events inside each spyd step
     - why measureDuration should not vary based on number of steps (i.e. measures sum of all steps):
        - splitting a step should not change the sample target duration
        - better responsiveness

Progress/reporting:
  - start|end() for progress and/or reporters??? If state, e.g. server
  - merge progress reporters to normal reporters???
     - e.g. reporter.progress(...)
     - progress reporting is called more often, and faster, i.e. keep separate???
        - reporter.report() duration is taken into account for aggregation time
           - what if async???
     - what about reporters which might need progress bar, but no live reporting??? e.g. json reporter
     - what about multiple reporters??? e.g. json + tty reporters, both with progress???
  - allow progress and/or normal reporters to be async???
  - how to disable:
     - live reporting??? per reporter or for all???
     - progress reporting???
     - normal reporting???
  - Multiple reporters in the same terminal output at once???
     - live reporting too???
  - Progress reporters with no TTY output???

MAX_LOOPS:
  - use two arrays:
     - measures is the final one
     - extraMeasures:
        - used instead of measures when measures.length > MAX_LOOPS / 2
        - merged to measures when either:
           - extraMeasures.length > MAX_LOOPS / 2
           - combination ended
        - merge with ever-decreasing weight:
           - before each merge, increase extraWeight integer from 1
           - pick NUM measures and NUM2 extraMeasures
              - NUM2 = Math.round(extraMeasures.length / extraWeight)
              - NUM = (MAX_LOOPS / 2) - NUM2
           - use quantiles
              - for both arrays, always start with min and end with max, with even spaces between picked items
           - concatenate + adaptive sort
  - have a little buffer, doing the merge a little before extraMeasures is full, so the last process does not have very short maxLoops or maxDuration???
  - pass limit to runners too???
     - with number of loops left???
     - add a maxTimes??? Or named maxLoops???
        - or add estimated duration to maxDuration instead???
           - e.g. maxLoopsMore: maxLoopsLeft / CONF.concurrency
           - is it needed??? would it be ok if measures were slightly over the limit???
     - is it even a concern due to maxDuration???
  - ensure the computed `stats.loops` is correct even when going over MAX_LOOPS
  - how does this work considering measures are split between `measures` and an array of not-merged-yet `childMeasures`???
     - done during aggregation???
  - lower limit due to several combinations and sampleGroups at same time???
     - each combination should get same max amount, so they don't influence each other (except for the number of combinations making that max amount smaller)
  - would above change the measure profile, which defeats the point of longer benchmarks???
  - pick elements to remove randomly instead of using quantiles, so it keeps statistical profile
     - random pick should have a uniform distribution. E.g. Math.random() entropy is less than 1e7, i.e. many indexes will have 0 probability to be picked
  - alternative: just stop iteration
  - max length (1e8) also applies to HTTP body (unless streaming used), i.e. limit might need to be lower inside each sample???

Concurrency:
  - run tasks in parallel inside the runner process instead of spawning several processes in parallel???

Try to see if anything more can be removed from `node` runner, i.e. simplified for all runners

Abstract runner orchestration so it is easier to create a runner???
   - how???
      - maybe an execute which maintains a state machine (using combination.id) and returns next action???

How to handle live reporting with big output???
  - At the moment, not possible to scroll
  - scrolling:
     - listen for UpArrow/k and DownArrow/j
     - keep track of current scroll integer
     - remove first and last lines before printing according to scrolling
     - when either stopping or ending benchmark, should show in full

Precision stat???
  - means difference between another benchmark with same settings
  - as opposed to standard deviation is variation between measures
  - how to compute???

Rethink making the history file a builtin store instead???
  - what's the directory of history.ndjson now that CONF.settings is removed???
     - common dir of task files???
     - CONF.save argument???
     - CONF.store.STORE.output PATH???
  - rename CONF.store to CONF.history???

Allow multiple stores???
  - At the moment, CONF.stores is an array, but only the first one is used

Programmatic usage???

REPL
  - maybe focus on simplicity and direct feedback with fewer features
    - no progress reporting
    - reporter much simpler, only one line
    - no inputs
    - no systems
    - no before|after???
  - alternative to having a REPL: CONF.run.{runnerId}.inline STR???

Localization:
  - CONF.lang "LANG"
     - e.g. "en_US"
     - def: "en_US", not guessed???
 - changes number formatting
 - translates text in reporters:
    - for common text like "Memory"
    - reporters use a translation utility which uses some shared translations, which can be augmented by contributors (not users)
 - translates titles:
    - allow titles.yml to optionally be OBJ: LANG: OBJ2
    - i.e. defined by users
 - what about units???
    - known units, e.g. duration
    - user-defined units

Slogan???
  - Current: Simple and precise benchmarking
  - Potential issues:
     - "benchmarking" might not be clear enough
     - remove "simple"??? there are simpler alternatives, e.g. just using for loops or console.time()
     - not enough stress on the great DX
        - use word "fun"??? "pretty"???

-------------------------------


Fix `exec`
  - e.g. `stdio` must be [`ignore`, `inherit`, `inherit`]
  - pSetTimeout() should be either unref() or clearTimeout(), otherwise it will hold the process
     - take into account that duration might be 0 or 1
  - should show ids, not titles

Fix CLI runner
  - remove all template variables:
     - user-defined
        - can use either subshells, or command wrapper instead
     - {{ENV_VAR}}
        - can use environment variable in command instead
     - step variables
        - steps should communicate with filesystem instead
     - inputs
        - should be passed as environment variables instead of template variables
  - use stdout|stderr "ignore"

Precision:
  - compare precision:
     - compare with multiple processes:
        - difference between combinations of single benchmark, vs between single combinations of different benchmarks
        - median vs standard deviation vs variation between processes
        - small CONF.duration vs big CONF.duration
  - note: using a "for loop" without spyd does work:
     - increasing the count makes the results more and more precise
     - the first 2 loops (regardless of the total count) always seem to be different from others
  - check if using a fixed, low amount of processes helps with precision???
  - find ways to improve precision even more???

Live updating:
  - steps:
     - reporter.report()->STR:
        - done after each measures aggregation:
           - counted as part of the `aggregateDuration`
           - unless calibrated true
              - reason: if calibrated true, measures will be removed, which would create confusing reporting (e.g. for min|max)
              - other reason: during calibrated true, stats change a lot, creating flicker
     - do I/O using the last reporter.report() STR
        - done during progress reporting interval function
           - not counted as part of the `aggregateDuration`
  - both steps are:
     - also initially done right after combinations are available, showing no stats yet
     - only done if all of:
        - reporter.progress true (def: false)
        - reporterConfig.output "-"
        - CONF.quiet false
  - call reporter.report()->STR with same arguments as the final reporter.report():
     - including result being normalized
     - including having information from initial listResults() (e.g. for previous combinations and diff)
     - including all possible result.combinations, even ones not measured yet
     - excluding CONF.show, always empty
  - if reporter.progress false, reporter.report() can be async. If true, cannot.
  - progress reporter is appended to it:
     - performed right after each live reporting
  - limit the amount of changes of the general shape of the output between the initial call and the final one, to make it look nicer
     - e.g. tables should be initially shown with all rows|columns and empty cells
  - when:
     - ending benchmark: clean progress reporting. Report again (do not keep the last live reporting)
     - stopping benchmark once: clean progress reporting. Report again (do not keep the last live reporting)
     - stopping benchmark twice: clean progress reporting. Keep last live reporting

CONF.concurrency NUM
  - validate that CONF.concurrency NUM is integer >=1
  - each sample spawns NUM processes in parallel
     - in `bench` command, but not in `exec` command
     - start|end group of processes together
     - use same `params`, including `maxLoops`
     - if one process fails
        - the other ones should continue (for cleanup)
        - but the sample should then propagate error
     - measureCost computation should use the same amount of parallel processes
  - add code comment that:
     - CONF.concurrency is meant to measure cost of parallelism
        - both CPU and I/O parallelism
     - if task is I/O bound, it can also improve precision by performing more measures, at the cost of accuracy (due to cost of parallelism)
        - the number where parallel processes start competing for CPU depends on how much duration the task spend on CPU vs I/O
        - above that number:
           - median measure increases much more
           - precision decreases much more
     - move the current code comment from src/measure/combination.js (about spawning processes serially)
  - handle spawn errors due to too many processes at once
     - try to remove process limit with ulimit, and see if another error can happen with a high CONF.concurrency, e.g. too many open files

isAsync:
  - initial check for isAsync:
     - execute func once, without await
     - check if return value is promisable (using p-is-promise)
     - sets func.isAsync BOOL (originally undefined)
     - if isAsync, await return value
  - do the above when func.isAsync undefined && repeat 1
     - add code comment that repeat should always be 1 when func.isAsync undefined, and this probably won't change. It is more of a failsafe.
  - do the above in a `sync_async` dir, next to `sync` and `async` dirs
  - do the above independently for beforeEach, main and afterEach
  - always use await on beforeAll|afterAll, i.e. allow both sync and async
  - remove task.async BOOL

Quantiles|histogram:
  - persist in stores
  - some stats should have a space-efficient shape for stores, but be denormalized on load:
     - histogram:
        - denormalized: OBJ_ARR: low, high, frequency
        - normalized: ARR of [high, frequency]
     - quantiles:
        - denormalized: OBJ_ARR: percentage NUM, value NUM
        - normalized: NUM_ARR
     - both: use difference from median in durations
  - show in `debug` reporter

reporter.debugStats BOOL
  - def: false
  - true for `debug` reporter
  - if false, do not pass:
     - mean
        - add comment that we must ensure median is the main one used, so different reporters are consistent, and also because it is used in sorting combinations, and also it is less precise
     - times
        - add comment that it is a bad indicator of precision, and also might be confused as an indicator of speed due to other benchmark libraries showing it like that
     - minLoopDuration, samples, repeat, loops

Sort all the code comments in `measure`, `sample`, `stats`, `process` and the `node|cli` runners. It's a bit messy right now, and some might be outdated

plugin.config.PROP STR[_ARR]
  - for all plugins: reporter, progress, runner, store
  - meant for validation, using jest-validate (same validation as for CONF.*):
     - validate against unknown props
        - including automatic suggestion when using wrong case
     - if ARR, use multipleValidOptions()
  - required to use PROP
  - validate PROP matches /^[a-z][a-zA-Z\d]*$/
     - i.e. use - (not _ nor case) as delimiter
  - if STR starts with ./ ../ or /, add to `PATH_CONFIG_PROPS` during config file path resolution

Variations:
  - some config properties can be optionally variable:
     - value PROP: { ID: VAL, ... } instead of VAL
     - PROP: variable property (whole combination category)
     - PROP + ID: variation
  - when merging results:
     - if some PROP uses variations in one result, but not in another
     - then use ID 'default' to convert so all instances of that PROP have a variable property
     - this allows keeping history continuity when introducing variations
  - only on any CONF.* that can change the results:
     - CONF.concurrency
     - CONF.inputs.{inputId}
     - not CONF.duration:
        - no reasons why users would want to measure with different CONF.duration
        - complicates implementation
     - any CONF.runner.{runnerId}.PROP
        - cartesian product only to combinations with that runner
  - each variable property is a separate combination category, distinct from others:
     - in CONF.include|exclude
     - in combination.columnName: several STR
     - result.variations OBJ_ARR_ARR, not OBJ_ARR
     - combination.variations ARR
  - variationId:
     - 'PROP.ID' (not just 'ID')
        - full property name
           - including `inputs.*` and `runnerId.*`
     - used as identifier like for other combination categories, including:
        - comparing between combinations (including between different benchmarks)
        - selecting with CONF.include|exclude|limit
     - no duplicate ID check:
        - reason: namespaces by property name, i.e. not needed
     - should validate ID (not full variationId) allowed characters using the same validation as taskId|systemId|inputId
  - variationTitle: default to same as id
  - variationValue: property VAL
  - set on result.* like other combination categories:
     - for combinations sorting, mean, rank, selection, etc.
     - result.variations OBJ_ARR_ARR: id STR, value VAL, title STR
     - combination.variations OBJ_ARR: id STR, value VAL, title STR
  - just like tasks and inputs, only reported in main reporting table, not in system info below it
  - if several variations have different runner.versions.VAR VAL, they are concatenated as a result.systems[*].versions.VAR ARR
     - printed as a comma-separated list by reporter prettify logic
     - does not mention which variation used which ones. It should be obvious enough from ids or titles
  - multiple inputs combination categories using variations:
     - i.e. CONF.inputs.inputId.ID VAL
     - input combination category is just like any other variation combination categories
        - including for cartesian product, CONF.include|exclude|limit
        - variationId is 'inputs.inputId.ID'
           - reasons we do not use shorter 'inputId.ID' instead:
              - forward compatiblity with future variable configuration properties
              - consistent/monomorphic, i.e. simpler to learn
           - like any variationId: no duplicate ID check
           - including with CONF.titles
     - inputId is only part of the variationId
        - used in `inputs` passed to runner.measure()

combination.rowName|columnName:
  - remove combination categories where all combinations have same value:
     - exception: if all combination categories removed, show only task
  - rowName: task, step
  - columnName order is system, runner, input, variation
  - if there are runnerConfig variable properties:
     - must be last
     - empty values (when on different runners) removed
        - i.e. different variable properties for different runners might be aligned together
     - columnName arrays should all have same length, by appending empty strings
        - i.e. if one runner has 1 variable property, and another has 2, the first gets an empty string appended
  - fix combination error task prefix, so it shows all the combination categories
     - except the ones not useful, e.g. system (since there is only one per execution), or inputs if none are used, etc.
  - use in `taskPrefix` used when there is a error during measuring???
  - fix `name` in `compare/limit.js`

Stats logic:
  - bug: serializePercentage() would show -0.5% as 0.5%???
  - unitless counts should use exponential notation when possible, with 3 significant digits
     - i.e. no need for thousands separators

CONF.since:
  - problem: merging sinceResult.systems with result.systems
     - because current code assumes previous results have only a single system OBJ
     - probably:
        - normalize all `previous` system OBJ to systems [OBJ]
           - unless already normalized (for sinceResult)
        - i.e. no need to differentiate in code
        - put sharedSystem away, so it does not apply to sinceResult
  - try to see if sinceResult can be computed during listResults() instead, so it is validated quickly

Delta formats:
  - value is always a minimum, i.e. use >=
     - reason: allows specifying a larger "since target" (e.g. high NUM or very old date) without error
     - including absolute|relative date|time
     - absolute date: should be start of day
     - count: if too high, return first result
     - git commit|tag|branch: return most recent instead, because git refs are meant to be pointers to last commit, and users most likely intend to compare with last result of ref
  - if >= target result
     - if:
        - show|remove delta: error
        - CONF.since delta:
           - sinceResult undefined
           - result.previous empty ARR
           - no stats.diff
           - no combinations merging
     - happens with formats:
        - date|time > last previous result
           - including through git ref resolution
        - count 0
           - negative should error though
     - this includes when there are no previous results

Improve date delta:
  - learn day.js
  - parse absolute durations using day.js
  - add relative duration (like "3m") using day.js
     - add it to CLI help messages for `show` command and `since` properties
     - use the closest result that is after (not before) the resolved timestamp
        - this is how absolute date/time delta format works too

Stores:
  - add comment that no support for multiple stores because not very useful and bring questions around data sync
  - add comment that no need to make addResults() concurrent safe. Reasons:
     - append will be concurrent safe on many situations since the number of bytes to write is fairly small
     - unlikely two benchmarks would end exactly at same time on same machine
     - not good practice to execute two benchmarks at same time on same machine
     - can easily fix it by doing a `sync`
  - store.start(storeConfig) -> FUNC(storeConfig)->store instead (closure)
  - always saved at SETTINGS/history.ndjson
  - results use ndjson format. Reasons:
     - JSON parsing much faster than YAML
     - appending a file is much faster than writing the whole file
  - results file vs store:
     - results file logic is not considered a store
        - built-in, not configurable except for file location
        - not loaded like the stores
        - uses functions: addResult(), listResults(), removeResult(), syncResults()
           - result file location is passed as argument (no start())
     - listResults() at the beginning of `bench|show|remove`
        - load full|unfiltered data
     - if CONF.save true:
        - await store.add()
        - if no error, then do addResult()
     - `remove` command:
        - await store.remove()
        - if no error, then do removeResult()
     - `sync` command:
        - store.list()->OBJ_ARR
           - load full|unfiltered data
        - then syncResults(OBJ_ARR)
           - no more store.replace() method

Reporting on `remove` command:
  - call reporters, like `show`
     - i.e. must accept same CONF.*, e.g. CONF.tasks|system
  - then prompt for removal confirmation
     - unless CONF.force true
  - do not print that confirmation removal suceeded (but print error if failed)

CONF.run-cli.shell:
  - instead of taskFile.shell
  - STR instead of BOOL: "none", "sh", "bash"
  - should default to "none" instead
  - `cli` runner.versions:
     - if "none": none
     - if "bash|sh": { Bash|sh: 'X.Y.Z'}

Plugin shape should be validated

Error handling:
  - better way for all plugins (report, progress, stores, runners) to signal user error vs bugs
  - better handling of child process errors due to runner bugs (handled as user error right now)
  - plugin|core errors should print message to report GitHub issues to the plugin|core
     - it should include system information

CONF.debug BOOL
  - add debug information, for bug reports
  - add to issue template
  - for all commands
  - print:
     - resolved config
     - task files
     - runner.versions
     - combinations
     - each sample's state (including maxDuration, repeat, etc.)
     - last result, new result
  - do not call reporters

Learn package 'simple-statistics' and use it in spyd???

When killing child process, should kill descendants too
  - e.g. with spyd-runner-cli and command 'yes', 'yes' keeps executing after timeout

Consider lowering the valid Node version for spyd-runner-node, so that `run.node.versions` can target lower versions

Add REPL to evaluate Node.js or `cli` runner task on-the-fly???
  - should allow OPTS.save, so differences are shown
  - or instead re-use shell REPL, and just allow one-off commands
     - e.g. spyd fast "CLI command"...
       or spyd fast "Node statements"...
     - each STR is a task

Create a store that works in GitHub actions???

Add progress reporters:
  - spinner with task name and current median
  - progress bar
  - both above (def)

Reporters:
  - types:
     - JSON
     - CLI list
     - CLI table
     - Markdown list
     - Markdown table
     - CLI graphs|histograms
     - CLI where the tasks are in both axis, and the cells are the difference in %
     - CLI with horizontal bars for medians
        - with full CLI width for slowest median
        - still show numbers on top of bars (or on their left side)
        - def reporter instead of simple CLI list, except when there is only one combination
        - for Markdown too???
     - HTML
     - CLI time series (with previous combinations)
  - CLI|Markdown list:
     - follow the formatting I used in fast-cartesian example
        - simple list for TASK with no inputs
  - CLI|Markdown tables:
     - inputs as x axis, tasks as y axis
  - default reporter:
     - CLI|Markdown table if more than half of cells would be filled, and some inputs are defined
        - CLI|Markdown list otherwise
     - Markdown table|list if CONF.insert '*.md|*.markdown|README|readme'
        - CLI table|list otherwise

Make `precise-now` work on browser + node

Split `precise-timestamp` to own repository
  - make it work on browser + node
  - problem with browser: performance.now() is made only ms-precise by browser due to security timing attacks

Separate into different repos:
  - some plugins are builtin, i.e. required as production dependencies by core
     - including spyd-run-node and spyd-run-cli (until more runners are created)
  - types: spyd-reporter|progress|runner|store-*
  - spyd -> spyd (CLI) + spyd-core (non-CLI)

Clarify features in `README`:
  - most precise and accurate benchmarking
  - pretty reporting
  - history
  - performance testing
  - automatically insert latest results into your documentation
  - custom reporters
  - TypeScript support
  - CI-friendly

Add tests, documentation, etc.:
  - for all repos, including sub-repos
  - add keywords (GitHub, package.json)

Use key-value abstraction layers to add more built-in spyd-store-*

Utilities to help people creating reporters, runners, progress reporters, stores
  - GitHub template
  - test utility

Competitors benchmark:
  - benchmark with other benchmarking tools
  - each should measure Math.random() for the same duration
     - use different durations as inputs
  - report both medians (for accuracy) and standard deviation (for precision)

Add roadmap:
  - point to it from contribution doc to orient contributors towards features I want (e.g. HTML reporter)

Send PRs to do or redo benchmarks of repositories to
  - get user feedback
  - experience the library as a user
  - get visibility

Promote

Add other runners:
  - spyd-runner-chrome (maybe using puppetter)
  - spyd-runner-firefox (maybe using puppetter-firefox)
  - spyd-runner-selenium
  - spyd-runner-bash
  - spyd-runner-go

Commercial offer:
  - reporting dashboard:
     - show time series (i.e. keep history)
        - should not lose history when change only the `title` of the function|variant
     - nice data visualization
     - should show function bodies
  - code editor for tasks files:
     - perform benchmark (inside browser not on our servers)
     - send results to API (like what users would do on CI otherwise)
     - show results from API
     - i.e. can be used like jsperf
  - Sharing like jsperf:
     - allow users to benchmark in their own browsers
  - PR bot
  - notifications
  - user should report results to our API
     - like Codecov does
     - i.e. we do not pay for infrastructure cost, except simple CRUD API to store results
     - should be integrated with CI (i.e. use `ci-info`)
  - pricing:
     - free for open source
     - pay per private repo
