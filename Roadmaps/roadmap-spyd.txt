
        
   SPYD  
        



Instead of `skip`, use a "stop" event
  - child should wait for it with Promise.race()
  - in the other side of that Promise.race(), child should iterate over 0-Infinity "run" events

Try to move as much non-language-specific logic as possible from node-runner:
  - e.g. taskTitle and variariationTitle default values

Refactor run/main.js (split into several functions/files)

failOnExit() in end.js is still fired even on success because it's run in Promise.race() (which does not cancel promises)

Replace process IPC by JSON stream on stdin|stdout
  - e.g. { event: 'load', taskFile, skip BOOL }

OPTS.tasks "TASK_ID"_ARR:
  - def: all

OPTS.variations "ID"_ARR
  - def: all

Runners:
  - own packages
  - distributed with npm, JavaScript endpoint, but can otherwise use any programming language
  - entry point must export RUNNER_OBJ:
     - extensions "EXT"_ARR:
        - chooses runner to pick
        - several runners can be picked, resulting in several separate iterations
           - e.g. node + browsers
     - command(RUNNER_OPTS)->OBJ_ARR
        - OBJ_ARR:
           - title STR: shown in reporting
              - not shown if all iterations have same commandTitle
           - command STR: to launch runner entry point
     - versions.RUNTIME "COMMAND"
        - e.g. { node: "node --version" }
        - set to BENCHMARK.system.RUNTIME
        - also used to check if runtime exists
  - OPTS.run.RUNNER OBJ
     - chooses runners and pass options to them
        - RUNNER is module name
     - passed to child as EVENT.options OBJ during 'load' event
  - OPTS.require -> OPTS.run.node.require
  - BENCHMARK.runners OBJ_ARR:
     - OBJ_ARR: runnerId, runnerTitle
     - sorted by fastest average median
  - ITERATION.runner INDEX:
     - pointing to BENCHMARK.runners[INDEX]
  - should behave like it was an extra level of variations:
     - ITERATION.fastest should handle likewise
     - ITERATION.fullName STR (task + variation + runner) vs ITERATION.name (variation + runner)
  - spyd-run-node
     - separate from browsers
     - supports TypeScript
  - add other runners:
     - spyd-run-chrome (maybe using puppetter)
     - spyd-run-firefox (maybe using puppetter-firefox)
     - spyd-run-selenium
     - spyd-run-bash

spyd-node comparing node versions:
  - OPTS.run.node.versions 'VERSION,...'
  - VERSION is 'vMAJOR[.MINOR[.PATCH]]'
  - run one iteration per node version
  - try to use "-r node" (node-bin-gen)

OPTS.data 'DIR':
  - def: "PACKAGE_ROOT/spyd/"
  - save file to DATA_DIR/TIMESTAMP.json.gz:
     - TIMESTAMP: DATE.toISOString()
     - compressed with Gzip
  - same content as JSON reporter
     - including all stats
  - can be HTTP[S] URL:
     - GET / -> BENCHMARK_ARR
     - POST BENCHMARK
        - on --save
  - BENCHMARK.iterations[*].previous BENCHMARK_ARR
     - not persisted to data file, but passed to reporters
     - data files are loaded in parallel

OPTS.save BOOL (def: false):
  - save current results to data file

BENCHMARK.iterations[*].stats.diff NUM:
  - percentage of change of median since latest data file
     - for each iteration, use most recent data file that has this iteration (taskId + variationId + runnerId)
     - undefined if no previous data file
  - printedStats.diff '+|-NUM%'
     - padded (between +|- and NUM) so that all iterations[*].diff have same length
  - OPTS.diff BOOL|"TIMESTAMP" (def: true):
     - stats.diff undefined if false
     - if "TIMESTAMP" use that data file instead
  - not persisted to data file, but passed to reporters
  - shown by reporters, e.g. green|red down|up arrow with time difference
  - allow comparing data files with different BENCHMARK.system but warn on reporting using:
     - BENCHMARK.previousSystem OBJ:
        - reported as "Previous system:" after "System:"
        - only keep fields that are different
        - undefined if no differences
     - BENCHMARK.system.id 'HASH'
        - used by time series reporter to distinguish different systems

OPTS.limit[.TASK_ID[.VARIATION_ID[.RUNNER_ID]]] NUM
  - NUM is max ITERATION.stats.diff
     - def: twice standard deviation
  - can be specified several times
  - TASK_ID|VARIATION_ID|RUNNER_ID:
     - can be *
     - default to *
  - ITERATION.slow BOOL
     - used by reporters
        - no CLI error message
     - not persisted to data file, but passed to reporters
  - if any ITERATION.slow true, exit code 1

OPTS.show BOOL|"TIMESTAMP" (def: false)
  - re-use data file instead of doing new run
  - if true, use latest data file
  - the following becomes relative to the data file timestamp:
     - OPTS.diff true "latest data file"
     - OPTS.limit
     - BENCHMARK.iterations[*].previous

Add progress reporters:
  - spinner with task name and current median
  - progress bar
  - both above (def)

Reporters:
  - types:
     - JSON
     - CLI list
     - CLI table
     - Markdown list
     - Markdown table
     - CLI graphs|histograms
     - CLI where the tasks are in both axis, and the cells are the difference in %
     - CLI with horizontal bars for medians
        - with full CLI width for slowest median
        - still show numbers on top of bars (or on their left side)
        - def reporter instead of simple CLI list, except when there is only one iteration
        - for Markdown too???
     - HTML
     - CLI reporter that shows BENCHMARK.iterations[*].previous as a time series
  - CLI|Markdown list:
     - follow the formatting I used in fast-cartesian example
        - simple list for TASK with no variations
  - CLI|Markdown tables:
     - variations as x axis, tasks as y axis
  - default reporter:
     - CLI|Markdown table if more than half of cells would be filled, and some TASK.variations are defined
        - CLI|Markdown list otherwise
     - Markdown table|list if OPTS.insert '*.md|*.markdown|README|readme'
        - CLI table|list otherwise
  - should use iteration.fastest BOOL for highlighting
  - Markdown|HTML reporters should add "Benchmarked with [spyd](https://github.com/ehmicky/spyd)" when REPORTER_OPTS.link true

BENCHMARK.ci OBJ:
  - CI information using library like ci-info
  - used e.g. by HTML reporter

BENCHMARK.git OBJ:
  - git information (commit, branch, etc.)
  - only if `git` installed
  - used e.g. by HTML reporter

REPL???

Competitors benchmark:
  - benchmark with other benchmarking tools
  - each should benchmark Math.random() for the same duration
     - use different durations as variations
  - report both medians (for accuracy) and standard deviation (for precision)

Separate each reporter to own package
  - builtin reporters are required as production dependencies by core

Separate now.js to its own package

Separate resolution.js to its own package

Separate stats to its own package
  - compare with existing packages

Separate measuring part to its own package
  - child processes might be able to spawn only this instead of the full spyd package

Programming language agnosticism

Add roadmap:
  - point to it from contribution doc to orient contributors towards features I want (e.g. HTML reporter)

Promote:
  - add keywords (GitHub, package.json)

Features:
  - most precise and accurate benchmarking
  - pretty reporting
  - comparison with previous benchmarks
  - performance testing
  - automatically insert latest benchmarks into your documentation
  - custom reporters
  - TypeScript support

Commercial offer:
  - reporting dashboard:
     - show time series (i.e. keep history)
        - should not lose history when change only the `title` of the function|variant
     - nice data visualization
     - should show function bodies
  - code editor for benchmark files:
     - run benchmarks (inside browser not on our servers)
     - send benchmark results to API (like what users would do on CI otherwise)
     - show results from API
     - i.e. can be used like jsperf
  - Sharing like jsperf:
     - allow users to run in their own browsers
  - PR bot
  - notifications
  - user should report benchmark results to our API
     - like Codecov does
     - i.e. we do not pay for infrastructure cost, except simple CRUD API to store benchmark results
     - should be integrated with CI (i.e. use `ci-info`)
  - pricing:
     - free for open source
     - pay per private repo
