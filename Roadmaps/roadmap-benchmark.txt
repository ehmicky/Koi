
               
   CHECK-SPEED  
               



Find better name

CLI tool:
  - benchmark file should export OBJ instead of calling BENCHMARK(OBJ, OPTS)
  - CONF is either CLI flags or config file
  - add a Gulp task

TASK.async BOOL:
  - automatically guessed???
  - should use two different `measure()` functions, since checking if return value is a promise takes time, which skews timing
  - should run serially not in parallel, so it does not skew timing
  - use async-done or other abstraction

Timeout:
  - should not skew timing though

Before|after hooks

On errors:
  - if OPTS.strict true (def), exit code 1
  - RESULT.failure OBJ:
     - errors ERROR_ARR: discard ERRORs with same ERROR.type + message (but not stack)
  - try/catch block should include `performance.now()` so it does not skew timing

Add more stats in RESULT.duration:
  - mean NUM
  - average NUM
  - deviation NUM
  - variance NUM
  - min|max NUM
  - percentiles NUM_ARR
  - all NUM_ARR
  - margin of error, relative margin of error, standard error of mean

TASK.thresholds OBJ:
  - same members as RESULT.duration
  - if above threshold:
     - exit code 1
     - on RESULT.failure.thresholds OBJ (with actual time that failed)

Reporters:
  - take RESULTS as inputs
  - stream RESULTs, so that can provide progress bar

OPTS.platforms STR (use browserlist and PACKAGE.engines)
  - run in different browsers and Node versions
     - i.e. should probably enforce ESM
  - create one RESULT per platform, with different RESULT.system

RESULT.system OBJ: hardware and software info

Commercial offer:
  - Dashboard:
     - show time series (i.e. keep history)
        - should not lose history when change only the `title` of the function|variant
     - nice data visualization
     - should show function bodies
  - Sharing like jsperf:
     - allow users to run in their own browsers
  - PR bot
  - notifications
  - user should report benchmark results to our API
     - like Codecov does
     - i.e. we do not pay for infrastructure cost, except simple CRUD API to store benchmark results
     - should be integrated with CI (i.e. use `ci-info`)
  - pricing:
     - free for open source
     - pay per private repo
