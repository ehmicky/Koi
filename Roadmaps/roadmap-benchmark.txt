
                     
   ROADMAP-BENCHMARK  
                     



Current benchmark tools are not great

Use cases / features:
  - testing over several browsers + Node
  - regression testing
  - comparing with other tools
  - nice stats
  - nice reporting
  - comparison over time
  - should be very simple

Business idea:
  - dashboard
  - updated by CI
  - shows improvements over time
  - shows comparison with other tools
  - notifications

Use test-each

Commercial offer:
  - free for open source
  - pay per private repo or per task???
  - PR bot
  - user should report benchmark results to out API
     - like Codecov does
     - i.e. we do not pay for infrastructure cost, except simple CRUD API to store benchmark results

CLI tool

File should use ESM

Use CONF (CLI or file)

Should export OBJ:
  - key is "ID". Used to compare over time
  - value is TASK

TASK.main(): function being benchmarked

TASK.args ARGS_ARR:
  - call several times with different ARGS
  - prettyFormat() on ARGS to include in title

TASK.title STR
  - used by reporters
  - def to TASK.id

TASK.async BOOL:
  - auto guessed???
  - OPTS.maxConcurrency NUM
  - use async-done or other abstraction

CONF.repeat NUM

On errors:
  - if OPTS.strict true (def), exit code 1
  - set to RESULT.errors ERROR_ARR:
      - discard ERRORs with same ERROR.type + message (but not stack)

RESULT.duration OBJ:
  - mean NUM
  - average NUM
  - deviation NUM
  - variance NUM
  - min|max NUM
  - percentiles NUM_ARR
  - all NUM_ARR

TASK.thresholds OBJ:
  - same members as RESULT.duration
  - if above threshold:
     - exit code 1
     - on RESULT.failure.thresholds

RESULTS RESULT_ARR:
  - task TASK
  - args ARGS (each ARGS create different RESULT)
  - duration OBJ
  - failure OBJ:
     - errors ERROR_ARR
     - thresholds OBJ (with actual time that failed)
  - options OPTS

Reporters:
  - take RESULTS as inputs
  - stream RESULTs, so that can provide progress bar

OPTS.platforms STR (use browserlist and PACKAGE.engines)
  - run in browsers and Node

RESULT.system OBJ:
  - hardware and software info
