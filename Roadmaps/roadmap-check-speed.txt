
               
   CHECK-SPEED  
               



Find better name

Add keywords (GitHub, package.json)

checkSpeed(TASK_ARR, CONF)->RESULT_ARR
  - including CONF.repeat

TASK:
  - name STR (def: named export)
  - main(...args)
  - parameters OBJ_ARR:
     - name STR
     - values ARR[()]
        - done right before each main()
     - cleanup(...args)
        - done right after each main()

Run each main() first, then each parameter, so that table and lists can be printed incrementally

Reporters:
  - FUNC(RESULT_ITERABLE)
  - allow several reporters at once:
     - figure out how to make it work with ITERABLEs (should go through to_learn for streams and iterables)
  - types:
     - silent
     - CLI list
     - CLI table
        - def if at least two mains have at least one parameter with same name
           - what if no parameters???
     - Markwdown list
     - Markwdown table
  - CLI|Markdown tables:
     - parameters as x axis, mains as y axis

Async:
  - for main|values|cleanup()
  - automatically follow PROMISE if return one
     - but remain sync otherwise
  - should use two different `measure()` functions, since checking if return value is a promise takes time, which skews timing
  - run one main+parameters at a time, but each CONF.repeat for that one in parallel
  - CONF.maxConcurrency NUM
     - def: 100

CLI pointing to file:
  - doing named exports for each TASK:
     - TASK.name defaults to name of the export
  - CONF are CLI options
     - can be config file too
  - default to benchmarks.js or benchmarks/index.js or benchmarks/main.js

Add a Gulp task to my gulp-shared-tasks

CONF.timeout NUM (in secs)
  - for whole run
  - checked after each iteration
  - make whole run stop with exit code 1
  - RESULT.failure.timeout true on all RESULTs
  - def: 10 minutes

RESULT:
  - failed BOOL (if any failure.* set)

On errors:
  - if CONF.strict true (def):
     - make whole run stop with exit code 1
  - RESULT.failure OBJ:
     - errorsMessages 'ERROR'_ARR: discard ERRORs with same ERROR.name + message (but not stack)
     - errorsCount NUM
  - try/catch block should include `performance.now()` so it does not skew timing

Add more stats in RESULT.duration:
  - mean NUM
  - average NUM
  - deviation NUM
  - variance NUM
  - min|max NUM
  - percentiles NUM_ARR
  - all NUM_ARR
  - margin of error, relative margin of error, standard error of mean

CONF.thresholds OBJ:
  - same members as RESULT.duration
  - if above threshold:
     - exit code 1
     - on RESULT.failure.thresholds OBJ (with actual time that failed)

CONF.platforms STR (use browserlist and PACKAGE.engines)
  - run in different browsers and Node versions
     - i.e. should probably enforce ESM
  - create one RESULT per platform, with different RESULT.system

RESULT.system OBJ: hardware and software info

Commercial offer:
  - Dashboard:
     - show time series (i.e. keep history)
        - should not lose history when change only the `title` of the function|variant
     - nice data visualization
     - should show function bodies
  - Sharing like jsperf:
     - allow users to run in their own browsers
  - PR bot
  - notifications
  - user should report benchmark results to our API
     - like Codecov does
     - i.e. we do not pay for infrastructure cost, except simple CRUD API to store benchmark results
     - should be integrated with CI (i.e. use `ci-info`)
  - pricing:
     - free for open source
     - pay per private repo
