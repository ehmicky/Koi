
               
   CHECK-SPEED  
               



We only return one "average" metric:
  - as opposed to returning distribution and related metrics: variance, standard error, etc.
  - reasons:
     - users are mostly interested in this
     - simpler interface
     - distribution is only due to:
        - variation due to engine (garbage collection, optimization, etc.): not wanted in results
        - variation due to different input:
           - non-random set of inputs: should instead run several benchmarks with each set, instead of mixing several distributions
           - random set of inputs:
              - want distribution of this??? E.g. load testing with real input
              - could do the same logic as median but with percentile instead???

Use medians instead of arithmetic mean:
  - reason: remove outliers due to:
     - garbage collection and background processes
     - engine optimization over time (first runs tend to be much slower)

Use a recursive median of medians:
  - reason: O(log n) memory instead of O(n)
     - i.e. can have high number of iterations without crashing
  - use an outdegree of 3 because:
     - we want the smallest possible for minimum memory consumption
     - 2 would make an arithmetic mean instead of a median

Remove the time taken to perform iteration itself:
  - reasons:
     - performance.now() takes some time (36ns on my machine)
     - the iteration itself takes a little time (1ns on my machine)
  - do it by running an empty task:
     - then remove duration from it
     - should be at least 0 (in case function super fast)

Use max duration:
  - as opposed to fixed number of loops
  - reasons:
     - does not increase variance as engines or code gets faster
        - because faster code has higher variance
     - more stable user experience, avoiding potentially very long runs
     - get the most precision as user is ready to wait
     - no separate timeout feature needed

Timing duration:
  - check time at each loop:
     - after end is recorded, i.e. does not influence current iteration timing
     - compared against start of run
  - when duration reached, stop run
  - should do bottom-up, since we don't know how long this is going to run

Separate sync and async??? See differences once implemented

Automatically detect OS resolution???




OPTS.duration:
  - instead of OPTS.repeat
  - def 10 secs
  - how long to benchmark each task
  - also used for timeout
  - NUM (in secs) or STR (ms())

Report mean duration not ops/sec:
  - use lowest unit, with minimum 10 (i.e. 2 digits)
  - integers only

More input validation, including jest-validate

Child process???
  - launch the measurement/iterated code in a child process
     - communicate using IPC channel
  - pros:
     - better with OPTS.platform
     - better with OPTS.require
     - less skewing with hot paths v8 optimization
         - should check if this is true
  - cons:
     - slower: check how much
     - not possible with OPTS.tasks: must use OPTS.file
  - if fail on load, should bail whole run
  - worker threads instead???
     - cons:
        - time slicing might skew measurements
        - only Node, not other platforms
        - process.* has fewer methods
        - Node 10+
  - how many to launch:
     - one per main+parameters???
     - one per CPU??? Split main+parameters into smaller parts if more CPUs???
  - not compatible with OPTS.tasks (only with OPTS.file)
  - divide OPTS.concurrency by number of processes???

On thrown exceptions in main|before|after():
  - exit code 1
  - stop:
     - if OPTS.bail true (def: false), whole run
     - otherwise current main()+parameters repetitions
  - relies on p-times OPTS.stopOnError true (default value)
  - RESULT.failure OBJ:
     - error ERROR.stack
     - errorIndex NUM:
        - incrementing counter
        - used to display errors as footnotes during reporting
  - RESULT.failed BOOL: if any RESULT.failure set
  - try/catch block should include `performance.now()` so it does not skew timing

OPTS.require "PATH"_ARR:
  - call import() (transpiled to require()) before loading OPTS.file
  - goal: babel/register, etc.

Progress bar:
  - for any reporter except silent
  - throttle to only update progress bar once per NUMms (find highest NUM without making it jittery when OPTS.repeat is high)

Reporters:
  - FUNC(RESULT_ARR, REPORTER_OPTS)[->STR]
     - if STR, either printed to console or put in file (if OPTS.replace)
  - OPTS.reporters 'MODULE'_ARR
  - REPORTER_OPTS:
     - passed by user as OPTS.REPORTEROPT, passed to FUNC as OPTS.OPT
  - types:
     - silent
     - JSON
     - CLI list
     - CLI table
     - Markdown list
     - Markdown table
  - CLI|Markdown list:
     - follow the formatting I used in fast-cartesian example
        - simple list for TASK with no TASK.parameters
  - CLI|Markdown tables:
     - parameters as x axis, mains as y axis
  - default reporter:
     - CLI table if more than half of cells would be filled, and some TASK.parameters are defined
     - CLI list otherwise
  - sort RESULT_ARR:
     - sort main() from fastest to slowest (using average of parameters)
     - sort parameters() from fastest to slowest (using average among main() with same name)

REPORTER_OPTS.output 'PATH':
  - def: "-" (stdout)
  - if file contains string "benchtap-start" and "benchtap-end", insert instead:
     - insert content as is, with no wrapping
     - agnostic to:
        - format of the file to modify
        - format of the inserted content
        - users need to pick ones that make sense, e.g. Markdown reporters for Markdown files
     - looks for delimiters:
        - "check-speed-start" and "check-speed-end"
           - remove anything in-between
          - require both. Reason: when moving it around the file, user should be aware that both should be moved
        - there can be other things on the line (e.g. <!-- COMMENT --> or #COMMENT), but whole line is kept
        - no parsing needed

Add a Gulp task to my gulp-shared-tasks

Add more stats in RESULT.duration:
  - mean NUM
  - average NUM
  - deviation NUM
  - variance NUM
  - min|max NUM
  - percentiles NUM_ARR
  - all NUM_ARR
  - margin of error, relative margin of error, standard error of mean

Reporter showing advanced stats
  - e.g. graphs in terminal

OPTS.thresholds OBJ:
  - same members as RESULT.duration
  - if above threshold:
     - exit code 1
     - on RESULT.failure.thresholds STR_ARR (e.g. ['mean'])

RESULT.system OBJ: hardware and software info

OPTS.platforms STR (use browserlist and PACKAGE.engines)
  - run in different browsers and Node versions
     - i.e. should probably enforce ESM
  - create one RESULT per platform, with different RESULT.system
  - abstract to own package
  - should allow adding platforms as plugins

Promote:
  - add keywords (GitHub, package.json)

Commercial offer:
  - reporting dashboard:
     - show time series (i.e. keep history)
        - should not lose history when change only the `title` of the function|variant
     - nice data visualization
     - should show function bodies
  - code editor for benchmark files:
     - run benchmarks (inside browser not on our servers)
     - send benchmark results to API (like what users would do on CI otherwise)
     - show results from API
     - i.e. can be used like jsperf
  - Sharing like jsperf:
     - allow users to run in their own browsers
  - PR bot
  - notifications
  - user should report benchmark results to our API
     - like Codecov does
     - i.e. we do not pay for infrastructure cost, except simple CRUD API to store benchmark results
     - should be integrated with CI (i.e. use `ci-info`)
  - pricing:
     - free for open source
     - pay per private repo
