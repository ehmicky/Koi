
               
   CHECK-SPEED  
               



Find better name

Add keywords (GitHub, package.json)

checkSpeed(TASK_ARR, CONF)->RESULT_ARR
  - including CONF.repeat

TASK:
  - name STR (def: named export)
  - main(...args)
  - parameters OBJ_ARR:
     - name STR
     - values ARR[()]
        - done right before each main()
     - cleanup(...args)
        - done right after each main()

Run each main() first, then each parameter, so that table and lists can be printed incrementally
  - if no TASK.parameters, set no parameter name undefined in RESULTs/reporting

Reporters:
  - FUNC(RESULT_ITERABLE)
  - allow several reporters at once:
     - figure out how to make it work with ITERABLEs (should go through to_learn for streams and iterables)
  - types:
     - silent
     - JSON
     - CLI list
     - CLI table
     - Markdown list
     - Markdown table
  - CLI|Markdown tables:
     - parameters as x axis, mains as y axis
  - default reporter:
     - CLI table if more than half of cells would be filled, and some TASK.parameters are defined
     - CLI list otherwise
  - CONF.replace 'PATH':
     - update file
        - any format, but the inserted content is Markdown
     - looks for delimiters:
        - "check-list-start" and "check-list-end"
           - remove anything in-between
        - there can be other things on the line (e.g. <!-- -->), but whole line is kept
        - if only "check-list-start" is found, assume "check-list-end" is right after it
           - this allows documenting only adding "check-list-start" the first time
        - no parsing needed
     - for any reporter
        - each reporter must declare how to wrap it:
           - nothing (silent, Markdown)
           - ``` ``` (CLI)
           - ```TYPE ``` (JSON)

Async:
  - for main|values|cleanup()
  - automatically follow PROMISE if return one
     - but remain sync otherwise
  - should use two different `measure()` functions, since checking if return value is a promise takes time, which skews timing
  - run one main+parameters at a time, but each CONF.repeat for that one in parallel
  - CONF.maxConcurrency NUM
     - def: 100

CLI pointing to file:
  - doing named exports for each TASK:
     - TASK.name defaults to name of the export
  - CONF are CLI options
     - can be config file too
  - default to benchmarks.js or benchmarks/index.js or benchmarks/main.js

Add a Gulp task to my gulp-shared-tasks

CONF.timeout NUM (in secs)
  - for whole run
  - checked after each iteration
  - make whole run stop with exit code 1
     - stop RESULT_ITERABLE
  - RESULT.failure.timeout true on last result
  - def: 10 minutes

RESULT:
  - failed BOOL (if any failure.* set)

On thrown exceptions in parameters|main|cleanup():
  - exit code 1
  - stop:
     - if CONF.bail true (def: false), whole run
     - otherwise current main()+parameters repetitions
  - RESULT.failure OBJ:
     - error ERROR.stack
     - errorIndex NUM:
        - incrementing counter
        - used to display errors as footnotes during reporting
  - try/catch block should include `performance.now()` so it does not skew timing

Add more stats in RESULT.duration:
  - mean NUM
  - average NUM
  - deviation NUM
  - variance NUM
  - min|max NUM
  - percentiles NUM_ARR
  - all NUM_ARR
  - margin of error, relative margin of error, standard error of mean

CONF.thresholds OBJ:
  - same members as RESULT.duration
  - if above threshold:
     - exit code 1
     - on RESULT.failure.thresholds OBJ (with actual time that failed)

CONF.platforms STR (use browserlist and PACKAGE.engines)
  - run in different browsers and Node versions
     - i.e. should probably enforce ESM
  - create one RESULT per platform, with different RESULT.system

RESULT.system OBJ: hardware and software info

Commercial offer:
  - Dashboard:
     - show time series (i.e. keep history)
        - should not lose history when change only the `title` of the function|variant
     - nice data visualization
     - should show function bodies
  - Sharing like jsperf:
     - allow users to run in their own browsers
  - PR bot
  - notifications
  - user should report benchmark results to our API
     - like Codecov does
     - i.e. we do not pay for infrastructure cost, except simple CRUD API to store benchmark results
     - should be integrated with CI (i.e. use `ci-info`)
  - pricing:
     - free for open source
     - pay per private repo
