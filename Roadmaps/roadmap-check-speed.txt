
               
   CHECK-SPEED  
               



getStats() should use lighter version of computeStats() with only what's needed

Add before() and after()
  - when `repeat` is >1, should be performed NUM times
     - e.g. before() would return an ARR of length NUM, and we would fire main(ARR[count])

Add support for async:
  - first implement separately, then check if running sync code with async is ok

Algorithm rational:
  - problem: variance added by JavaScript engines
     - types:
        - garbage collection and background processes
        - engine optimization
     - solution: use medians instead of arithmetic mean
  - problem: performance slows down a lot periodically due to JavaScript engine background processes (e.g. garbage collection)
     - solution: remove slowest measurements
  - problem: bias and higher variance due to benchmarking logic:
     - types:
        - getting current timestamp (60ns on my machine)
        - the iteration itself:
           - checking for loop end (.3ns on my machine)
           - make function call, even if empty function (3ns on my machine)
     - solution: calculate bias by benchmarking an empty function, then remove that bias from actual benchmark
  - problem: higher variance due to benchmarking logic:
     - solution:
        - run fast benchmark in a loop to lower impact of timestamping
           - get the arithmeric mean of the loop
           - the number of loop repeats is automatically guessed
  - problem: measure time might be too low compare to time resolution to be precise
     - solution: looping (see above)
  - problem: first invocation of most functions is much slower than later runs, due to JavaScript engine optimization
     - of:
        - the benchmarked function
        - the benchmarking function itself (i.e. first task will run faster than other tasks)
     - solution: do cold starts
  - problem: the more a function is run, the faster is gets
     - solutions:
        - use bias calculation as a cold start
        - limit the maximum number of loops
           - when reaching a limit, start a new child process instead of keeping iterating
  - problem: function as fast as the iteration itself (approx. <1ns), cannot be measured because the iteration variance has too much influence
     - i.e. the result might be 0, and might vary a lot
     - solutions: none
     - invalid solution: loop unrolling
        - i.e. using `new Function('f', 'f();'.repeat(NUM))`:
           - or inline FUNC.toString() instead of 'f();'
        - issues:
           - has actually more variance, and is actually slower than loop
              - even when inlining FUNC.toString()
           - requires lots of memory, i.e. max NUM would be required and it would be quite low
           - odd engine optimizations
     - invalid solution: CPU profiling
        - i.e. using `node --cpu-prof`
        - issues:
           - use sampling approach, i.e. too much variance
           - min resolution 1 microsecond, i.e. almost never hit very fast functions
           - gives number of hits instead of number of nanoseconds
           - V8-specific
  - problem: using a fixed number of loops has issues
     - details:
        - faster code has higher variance:
           - does not increase variance as engines or code gets faster
           - when comparing fast tasks against slow tasks, make their variance more even
        - user cannot get as much precision as ready to wait
           - bad user experience when unexpectedly long task
           - require a separate timeout feature
     - solution: use max duration instead
  - problem: variation due to different processes having different performance profiles
     - details:
        - different processes: e.g. when end user is doing new invocations
        - each process runs with slightly faster/slower performance than others:
           - due to global/OS variation and JavaScript engine optimizations
     - solution: run several child processes and get a median of them
        - we run them serially:
           - otherwise they start to compete with each other, i.e. time is slower and variance higher
        - we don't use worker threads because:
           - time slicing would skew measurements
           - they run in a slightly different environment (process.* has fewer methods)

Add lots of comments in the code, especially describing algorithm and constants

Task file should be loaded by child process
  - should be done during child process load right away, since this is done in parallel for all child processes, and it might be slow

OPTS.duration:
  - instead of OPTS.repeat
  - NUM (in secs)
  - def 10 secs
  - minimum 1 second
  - how long to benchmark each task

Report mean duration not ops/sec:
  - use lowest unit, with minimum 10 (i.e. 2 digits)
  - integers only

Remove OPTS.tasks
  - reason: cannot be used with child processes since functions cannot be communicated to them
  - make OPTS.file a positional argument instead

Separate:
  - TASK.id:
     - used for stable identification
     - ESM exported variable name
  - TASK.name:
     - used for reporting
     - default to TASK.id

More input validation, including jest-validate

On thrown exceptions in main|before|after():
  - exit code 1
  - stop:
     - if OPTS.bail true (def: false), whole run
     - otherwise current main()+parameters repetitions
  - relies on p-times OPTS.stopOnError true (default value)
  - RESULT.failure OBJ:
     - error ERROR.stack
     - errorIndex NUM:
        - incrementing counter
        - used to display errors as footnotes during reporting
  - RESULT.failed BOOL: if any RESULT.failure set
  - try/catch block should include `performance.now()` so it does not skew timing
  - if child processes fail on load or has exit code, should bail whole run
     - ignore stderr???

OPTS.require "PATH"_ARR:
  - call import() (transpiled to require()) before loading OPTS.file
  - goal: babel/register, etc.

Reporters:
  - FUNC(RESULT_ARR, REPORTER_OPTS)[->STR]
     - if STR, either printed to console or put in file (if OPTS.replace)
  - OPTS.reporters 'MODULE'_ARR
  - REPORTER_OPTS:
     - passed by user as OPTS.REPORTEROPT, passed to FUNC as OPTS.OPT
  - types:
     - silent
     - JSON
     - CLI list
     - CLI table
     - Markdown list
     - Markdown table
  - CLI|Markdown list:
     - follow the formatting I used in fast-cartesian example
        - simple list for TASK with no TASK.parameters
  - CLI|Markdown tables:
     - parameters as x axis, mains as y axis
  - default reporter:
     - CLI table if more than half of cells would be filled, and some TASK.parameters are defined
     - CLI list otherwise
  - sort RESULT_ARR:
     - sort main() from fastest to slowest (using average of parameters)
     - sort parameters() from fastest to slowest (using average among main() with same name)

REPORTER_OPTS.output 'PATH':
  - def: "-" (stdout)
  - if file contains string "benchtap-start" and "benchtap-end", insert instead:
     - insert content as is, with no wrapping
     - agnostic to:
        - format of the file to modify
        - format of the inserted content
        - users need to pick ones that make sense, e.g. Markdown reporters for Markdown files
     - looks for delimiters:
        - "check-speed-start" and "check-speed-end"
           - remove anything in-between
          - require both. Reason: when moving it around the file, user should be aware that both should be moved
        - there can be other things on the line (e.g. <!-- COMMENT --> or #COMMENT), but whole line is kept
        - no parsing needed

Baseline reporting:
  - OPTS.baseline "PATH"|BOOL (def: sibling to benchmarked file, e.g. "benchmark.js" -> "benchmark.baseline.json")
     - if file exists, compare current results with baseline results and pass comparison to reporters
     - reporters should show it after the median, e.g. green|red down|up arrow with time difference
  - OPTS.save BOOL (def: false):
     - save current results to OPTS.baseline

"Benchmarks created by [check-speed](...)" in reporters:
  - OPTS.credits BOOL (def: true if insert mode)

Reporter showing advanced stats
  - e.g. graphs in terminal

Reporter where the tasks are in both axis, and the cells are the difference in %

Progress reporters:
  - separate from final reporters
  - OPTS.progress "MODULE"_ARR
  - called at regular intervals
  - called with:
     - percentage
     - current task
     - current task stats
 - types:
     - silent (def when OPTS.reporters 'silent')
     - progress bar
     - spinner with task name and current median
     - both above (def)

OPTS.limit.TASK_ID NUM
  - maximum median for that task
  - if above:
     - exit code 1
     - RESULT.failure.limit true

Add a Gulp task to my gulp-shared-tasks

RESULT.system OBJ: hardware and software info

OPTS.platforms STR (use browserlist and PACKAGE.engines)
  - run in different browsers and Node versions
     - i.e. should probably enforce ESM
  - create one RESULT per platform, with different RESULT.system
  - abstract to own package
  - should allow adding platforms as plugins

Separate now.js to its own package

Separate resolution.js to its own package

Separate stats to its own package
  - compare with existing packages

Separate measuring part to its own package
  - child processes might be able to spawn only this instead of the full check-speed package

Promote:
  - add keywords (GitHub, package.json)

Commercial offer:
  - reporting dashboard:
     - show time series (i.e. keep history)
        - should not lose history when change only the `title` of the function|variant
     - nice data visualization
     - should show function bodies
  - code editor for benchmark files:
     - run benchmarks (inside browser not on our servers)
     - send benchmark results to API (like what users would do on CI otherwise)
     - show results from API
     - i.e. can be used like jsperf
  - Sharing like jsperf:
     - allow users to run in their own browsers
  - PR bot
  - notifications
  - user should report benchmark results to our API
     - like Codecov does
     - i.e. we do not pay for infrastructure cost, except simple CRUD API to store benchmark results
     - should be integrated with CI (i.e. use `ci-info`)
  - pricing:
     - free for open source
     - pay per private repo
