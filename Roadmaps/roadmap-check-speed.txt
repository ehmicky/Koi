
               
   CHECK-SPEED  
               



Add keywords (GitHub, package.json)

Sync vs async:
  - compare
     - between:
        - current code
        - removing await in getDuration() and p-times
     - with empty main()
     - with very high OPTS.repeat
     - check:
        - total run time
        - average time for main()
  - if big difference:
     - split between sync and async versions of p-times + getDuration()
     - guess which one to use by triggering once main() then before() then after() and see the first that returns a PROMISE

Benchmark file:
  - OPTS.file "PATH"
  - first argument TASK_ARR -> OPTS.tasks TASK_ARR
     - has priority over OPTS.file
  - default to benchmarks.js or benchmarks/index.js or benchmarks/main.js
  - loaded with import() (transpiled to require())
  - does named exports for each TASK:
     - TASK.name defaults to name of the export
     - allow exporting a FUNC() as a shortcut to { main: FUNC }

Config file:
  - OPTS.config 'PATH'
  - merged to other OPTS with lower priority
  - defaults to findUp("check_speed.json", {cwd:path.dirname(OPTS.file)})
     - allow "check-speed.json" too

CLI:
  - can only use OPTS.file not OPTS.tasks

Child process???
  - launch the measurement/iterated code in a child process
     - communicate using IPC channel
  - pros:
     - better with OPTS.platform
     - better with OPTS.require
     - less skewing with hot paths v8 optimization
         - should check if this is true
  - cons:
     - slower: check how much
     - not possible with OPTS.tasks: must use OPTS.file
  - if fail on load, should bail whole run
  - worker threads instead???
     - cons:
        - time slicing might skew measurements
        - only Node, not other platforms
        - process.* has fewer methods
        - Node 10+
  - how many to launch:
     - one per main+parameters???
     - one per CPU??? Split main+parameters into smaller parts if more CPUs???

On thrown exceptions in main|before|after():
  - exit code 1
  - stop:
     - if OPTS.bail true (def: false), whole run
     - otherwise current main()+parameters repetitions
  - relies on p-times OPTS.stopOnError true (default value)
  - RESULT.failure OBJ:
     - error ERROR.stack
     - errorIndex NUM:
        - incrementing counter
        - used to display errors as footnotes during reporting
  - RESULT.failed BOOL: if any RESULT.failure set
  - try/catch block should include `performance.now()` so it does not skew timing

More input validation, including jest-validate

OPTS.timeout NUM (in secs)
  - for whole run
  - checked after each iteration
  - make whole run stop with exit code 1
  - RESULT.failure.timeout true on last result
  - def: 10 minutes

OPTS.require "PATH"_ARR:
  - call import() (transpiled to require()) before loading OPTS.file
  - goal: babel/register, etc.

Progress bar:
  - for any reporter except silent
  - throttle to only update progress bar once per NUMms (find highest NUM without making it jittery when OPTS.repeat is high)

Reporters:
  - FUNC(RESULT_ARR)[->STR]
     - if STR, either printed to console or put in file (if OPTS.replace)
  - OPTS.reporters 'MODULE'_ARR
  - types:
     - silent
     - JSON
     - CLI list
     - CLI table
     - Markdown list
     - Markdown table
  - CLI|Markdown list:
     - follow the formatting I used in fast-cartesian example
        - simple list for TASK with no TASK.parameters
  - CLI|Markdown tables:
     - parameters as x axis, mains as y axis
  - default reporter:
     - CLI table if more than half of cells would be filled, and some TASK.parameters are defined
     - CLI list otherwise
  - sort RESULT_ARR:
     - sort main() from fastest to slowest (using average of parameters)
     - sort parameters() from fastest to slowest (using average among main() with same name)

OPTS.replace 'PATH'_ARR:
  - update file
  - insert content as is, with no wrapping
  - agnostic to:
     - format of the file to modify
     - format of the inserted content
     - users need to pick ones that make sense, e.g. Markdown reporters for Markdown files
  - looks for delimiters:
     - "check-speed-start" and "check-speed-end"
        - remove anything in-between
        - require both. Reason: when moving it around the file, user should be aware that both should be moved
     - there can be other things on the line (e.g. <!-- COMMENT --> or #COMMENT), but whole line is kept
     - no parsing needed

Add a Gulp task to my gulp-shared-tasks

Add more stats in RESULT.duration:
  - mean NUM
  - average NUM
  - deviation NUM
  - variance NUM
  - min|max NUM
  - percentiles NUM_ARR
  - all NUM_ARR
  - margin of error, relative margin of error, standard error of mean

OPTS.thresholds OBJ:
  - same members as RESULT.duration
  - if above threshold:
     - exit code 1
     - on RESULT.failure.thresholds STR_ARR (e.g. ['mean'])

RESULT.system OBJ: hardware and software info

OPTS.platforms STR (use browserlist and PACKAGE.engines)
  - run in different browsers and Node versions
     - i.e. should probably enforce ESM
  - create one RESULT per platform, with different RESULT.system
  - abstract to own package
  - should allow adding platforms as plugins

Commercial offer:
  - reporting dashboard:
     - show time series (i.e. keep history)
        - should not lose history when change only the `title` of the function|variant
     - nice data visualization
     - should show function bodies
  - code editor for benchmark files:
     - run benchmarks (inside browser not on our servers)
     - send benchmark results to API (like what users would do on CI otherwise)
     - show results from API
     - i.e. can be used like jsperf
  - Sharing like jsperf:
     - allow users to run in their own browsers
  - PR bot
  - notifications
  - user should report benchmark results to our API
     - like Codecov does
     - i.e. we do not pay for infrastructure cost, except simple CRUD API to store benchmark results
     - should be integrated with CI (i.e. use `ci-info`)
  - pricing:
     - free for open source
     - pay per private repo
