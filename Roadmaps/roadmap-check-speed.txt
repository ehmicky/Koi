
               
   CHECK-SPEED  
               



Max memory size:
  - stop when array has around 100,000,000 elements
     - guess current system limit???
        - using os.*, process.report or v8.*???

Return full distribution:
  - check it with a main() with: Math.random() * (Math.random() > .5 ? (Math.random() > .5 ? 1: 10) :  (Math.random() > .5 ? 20: 30))
  - add:
     - percentiles NUM_ARR
     - margin of error, relative margin of error, standard error of mean
  - final returned array might be smaller size than max size if never reached the limit:
     - some stats (e.g. variance) should require minimum size
  - issue with `repeat`??? Probably transform any distribution to a bell curve
     - verify this
     - if so, some stats should not be calculated when `repeat` was used, because they would not be valid (e.g. percentiles)
        - however still make sense: figure out which ones: median, maybe arithmetic mean???
        - figure it by checking `count` and `loops`
  - test each stat:
     - e.g. average, mean, standard deviation, etc.
     - using main() with known distributions, based on Math.random, with a high maxDuration
     - do it locally while implementing to check everything is ok
     - also do it as unit tests
  - remove the part in "design" saying only mean is calculated, or instead say this is where the focus mostly is

Study node --prof and node --cpu-prof and see if that can be used for <1ns functions

Add before() and after()
  - when `repeat` is >1, should be performed NUM times
     - e.g. before() would return an ARR of length NUM, and we would fire main(ARR[count])

Add support for async:
  - first implement separately, then check if running sync code with async is ok

Design:
  - see if anything needs to be added here???
  - we only return one "average" metric:
     - as opposed to returning distribution and related metrics: variance, standard error, etc.
     - reasons:
        - users are mostly interested in this
        - simpler interface
        - distribution is only due to:
           - variation due to engine (garbage collection, optimization, etc.): not wanted in results
           - variation due to different input:
              - non-random set of inputs: should instead run several benchmarks with each set, instead of mixing several distributions
              - random set of inputs:
                 - want distribution of this??? E.g. load testing with real input
                 - could do the same logic as median but with percentile instead???
  - use medians instead of arithmetic mean:
     - reason: remove outliers due to:
        - garbage collection and background processes
        - engine optimization over time (first runs tend to be much slower)
  - use a recursive median of medians:
     - reason: O(log n) memory instead of O(n)
        - i.e. can have high number of iterations without crashing
     - use an outdegree of 3 because:
        - we want the smallest possible for minimum memory consumption
        - 2 would make an arithmetic mean instead of a median
  - remove the time taken to perform iteration itself:
     - reasons:
        - get current timestamp takes some time (45ns on my machine)
        - the iteration itself takes a little time:
           - time to increment counter (.3ns on my machine)
           - time to make function call, even if empty function (3ns on my machine)
     - i.e. allows precise measurement of nanoseconds-long functions
        - however if the function is as fast as the iteration itself (approx. <10ns), cannot measure because the iteration variance has too much influence.
           - the result then might be 0, and might vary a lot
  - if the task is very fast (approx. <0.1ms), also loop it and use arithmetic mean time:
     - reasons:
        - measured time must be too low compared to time resolution
           - we want at least 2 digits of precision
        - time taken to get current timestamp might be much slower than the task
           - i.e. much of the variance would be due to it, not to the task
           - we want task iteration to be at least 100 times the time required to get current timestamp
     - number of repetitions is automatically guessed
        - the same number is consistently used across the run
           - reason: looping a function might make it faster (due to engine optimizations)
     - alternative would be loop unrolling:
        - i.e. using `new Function('f', 'f();'.repeat(NUM))`:
        - pro: no bias of the iteration itself
        - cons:
           - requires lots of memory, i.e. max NUM would be required (probably 1e5, which is too low to estimate even ~50ns functions)
           - odd engine optimizations:
              - first call is much slower, i.e. requires a cold start
           - still need to calculate and remove the bias of calling that function itself (several nanoseconds bias)
           - cannot use a closure, i.e. function cannot reference upper scopes
  - use max duration:
     - as opposed to fixed number of loops
     - reasons:
        - faster code has higher variance:
           - does not increase variance as engines or code gets faster
           - when comparing fast tasks against slow tasks, make their variance more even
        - more stable user experience, avoiding potentially very long runs
        - get the most precision as user is ready to wait
        - no separate timeout feature needed
  - use bottom-up recursion:
     - reason: we don't know how many recursion levels are needed to reach max duration
  - run several child processes and get a median of them:
     - reasons:
        - when launching new processes (like when end user is running the command several times in CLI), variation is much
          higher than when using a single process and running several benchmarks in parallel
           - this is probably due to some engine optimization and global/OS variation
           - using a median over several child processes allow emulating user running those commands several times in CLI
     - we run them serially:
        - otherwise they start to compete with each other, i.e. time is slower and variance higher
     - we don't use worker threads because:
        - time slicing would skew measurements
        - only Node, not other platforms
        - they run in a slightly different environment (process.* has fewer methods)

Add lots of comments in the code, especially describing algorithm and constants

Task file should be loaded by child process
  - should be done during child process load right away, since this is done in parallel for all child processes, and it might be slow

OPTS.duration:
  - instead of OPTS.repeat
  - NUM (in secs)
  - def 10 secs
  - minimum 1 second
  - how long to benchmark each task
  - also used for timeout

Report mean duration not ops/sec:
  - use lowest unit, with minimum 10 (i.e. 2 digits)
  - integers only

Remove OPTS.tasks
  - reason: cannot be used with child processes since functions cannot be communicated to them
  - make OPTS.file a positional argument instead

Separate:
  - TASK.id:
     - used for stable identification
     - ESM exported variable name
  - TASK.name:
     - used for reporting
     - default to TASK.id

More input validation, including jest-validate

On thrown exceptions in main|before|after():
  - exit code 1
  - stop:
     - if OPTS.bail true (def: false), whole run
     - otherwise current main()+parameters repetitions
  - relies on p-times OPTS.stopOnError true (default value)
  - RESULT.failure OBJ:
     - error ERROR.stack
     - errorIndex NUM:
        - incrementing counter
        - used to display errors as footnotes during reporting
  - RESULT.failed BOOL: if any RESULT.failure set
  - try/catch block should include `performance.now()` so it does not skew timing
  - if child processes fail on load or has exit code, should bail whole run
     - ignore stderr???

OPTS.require "PATH"_ARR:
  - call import() (transpiled to require()) before loading OPTS.file
  - goal: babel/register, etc.

Reporters:
  - FUNC(RESULT_ARR, REPORTER_OPTS)[->STR]
     - if STR, either printed to console or put in file (if OPTS.replace)
  - OPTS.reporters 'MODULE'_ARR
  - REPORTER_OPTS:
     - passed by user as OPTS.REPORTEROPT, passed to FUNC as OPTS.OPT
  - types:
     - silent
     - JSON
     - CLI list
     - CLI table
     - Markdown list
     - Markdown table
  - CLI|Markdown list:
     - follow the formatting I used in fast-cartesian example
        - simple list for TASK with no TASK.parameters
  - CLI|Markdown tables:
     - parameters as x axis, mains as y axis
  - default reporter:
     - CLI table if more than half of cells would be filled, and some TASK.parameters are defined
     - CLI list otherwise
  - sort RESULT_ARR:
     - sort main() from fastest to slowest (using average of parameters)
     - sort parameters() from fastest to slowest (using average among main() with same name)

REPORTER_OPTS.output 'PATH':
  - def: "-" (stdout)
  - if file contains string "benchtap-start" and "benchtap-end", insert instead:
     - insert content as is, with no wrapping
     - agnostic to:
        - format of the file to modify
        - format of the inserted content
        - users need to pick ones that make sense, e.g. Markdown reporters for Markdown files
     - looks for delimiters:
        - "check-speed-start" and "check-speed-end"
           - remove anything in-between
          - require both. Reason: when moving it around the file, user should be aware that both should be moved
        - there can be other things on the line (e.g. <!-- COMMENT --> or #COMMENT), but whole line is kept
        - no parsing needed

Baseline reporting:
  - OPTS.baseline "PATH"|BOOL (def: sibling to benchmarked file, e.g. "benchmark.js" -> "benchmark.baseline.json")
     - if file exists, compare current results with baseline results and pass comparison to reporters
     - reporters should show it after the median, e.g. green|red down|up arrow with time difference
  - OPTS.save BOOL (def: false):
     - save current results to OPTS.baseline

"Benchmarks created by [check-speed](...)" in reporters:
  - OPTS.credits BOOL (def: true if insert mode)

Reporter showing advanced stats
  - e.g. graphs in terminal

Reporter where the tasks are in both axis, and the cells are the difference in %

Progress reporters:
  - separate from final reporters
  - OPTS.progress "MODULE"_ARR
  - called at regular intervals
  - called with:
     - percentage
     - current task
     - current task stats
 - types:
     - silent (def when OPTS.reporters 'silent')
     - progress bar
     - spinner with task name and current median
     - both above (def)

OPTS.limit.TASK_ID NUM
  - maximum median for that task
  - if above:
     - exit code 1
     - RESULT.failure.limit true

Add a Gulp task to my gulp-shared-tasks

RESULT.system OBJ: hardware and software info

OPTS.platforms STR (use browserlist and PACKAGE.engines)
  - run in different browsers and Node versions
     - i.e. should probably enforce ESM
  - create one RESULT per platform, with different RESULT.system
  - abstract to own package
  - should allow adding platforms as plugins

Separate now.js to its own package

Separate resolution.js to its own package

Separate measuring part to its own package
  - child processes might be able to spawn only this instead of the full check-speed package

Promote:
  - add keywords (GitHub, package.json)

Commercial offer:
  - reporting dashboard:
     - show time series (i.e. keep history)
        - should not lose history when change only the `title` of the function|variant
     - nice data visualization
     - should show function bodies
  - code editor for benchmark files:
     - run benchmarks (inside browser not on our servers)
     - send benchmark results to API (like what users would do on CI otherwise)
     - show results from API
     - i.e. can be used like jsperf
  - Sharing like jsperf:
     - allow users to run in their own browsers
  - PR bot
  - notifications
  - user should report benchmark results to our API
     - like Codecov does
     - i.e. we do not pay for infrastructure cost, except simple CRUD API to store benchmark results
     - should be integrated with CI (i.e. use `ci-info`)
  - pricing:
     - free for open source
     - pay per private repo
