
               
   CHECK-SPEED  
               



Repeat calculation:
  - should be Math.ceil(minTime / (median + loopBias))
  - remove the "if median === 0" block

Repeat moves too much: why??? Shouldn't the median become more stable with time???
  - do not discard all previous values if only one is odd???
  - only discard first values until `repeat` stabilizes???
  - stop adjusting `repeat` once it stabilizes???
  - use last measurement instead of median when calculating `repeat`???

Biases calculation:
  - nowBias should use:
     - repeat always 0
        - must use a while {} loop instead of do {} while for 0 to work
     - nowBias 0
     - loopBias 0
  - loopBias:
     - repeat undefined (i.e. adaptive)
     - nowBias use the actual value
        - i.e. must be computed after nowBias
     - loopBias 0
  - try to give them percentage of duration instead of fixed duration
     - maybe 5% each???
     - should substract it from duration passed to main benchmark
 - make biases calculation part of benchmark() instead of exporting them separately

Incremental sampling for max array size??? one for each iteration???
  - try to guess memory allowed to process using os.* or process.report
  - algorithm should always keep min and max
  - algorithm:
     - when reach max size, merge halves into one half (by removing each odd index)
         - except min/max, i.e. max size should be odd number. Including for subparts recursive merges
             - or instead keep them in a separate array maybe??? might introduce less bias
     - that half now has "precision 2"
     - only parts of same precision can be merged together
        - so the second half gets filled again but with "precision 1"
        - once filled its own halves (quarters of whole array) are merged, leaving a quarter empty
        - and do so on until only one element is left
        - then both halves are precision 2, so we can merge
        - and so on recursively
     - when duration run out, parts with lower precision merge fewer elements back to final array

Try to think again of very very fast functions, and try new Function() again

Distributions:
  - tweak algorithm to make it as precise as possible, it's still not super precise
  - if the precision is now better with this new method, check if can decrease some constants, e.g. minimum bias calculation times
  - remove the part in "design" saying only mean is calculated, or instead say this is where the focus mostly is
  - max array size:
     - hardcoded
        - should be as big as the amount of memory we are ok to spend
           - check actual memory usage using memory dump report core module
     - how to maintain it??? sample randomly at each iteration??? or sample half of values each time the limit is reached???
     - final returned array might be smaller size than max size if never reached the limit:
        - some stats (e.g. variance) should require minimum size
  - check it with a main() with: Math.random() * (Math.random() > .5 ? (Math.random() > .5 ? 1: 10) :  (Math.random() > .5 ? 20: 30))
  - issues with `repeat` (measuring in loops):
     - probably transform any distribution to a bell curve
        - verify this
     - if so, some stats should not be calculated when `repeat` was used, because they would not be valid (e.g. percentiles)
        - however still make sense: figure out which ones: median, maybe arithmetic mean???
        - figure it out by returning:
           - `count` NUM: number of times main() was run
           - `loops` NUM: number of times measure() was run
           - should discard the ones not used (due to `repeat` discarding)
  - test each stat:
     - e.g. average, mean, standard deviation, etc.
     - using main() with known distributions, based on Math.random, with a high maxDuration
     - do it locally while implementing to check everything is ok
     - also do it as unit tests

Add before() and after()
  - how does it work with loops???

Add support for async:
  - first implement separately, then check if running sync code with async is ok

Remove `depth` argument from timing functions, unless it's now needed

Add more stats in RESULT.duration:
  - mean NUM
  - average NUM
  - deviation NUM
  - variance NUM
  - min|max NUM
  - percentiles NUM_ARR
  - all NUM_ARR
  - margin of error, relative margin of error, standard error of mean

Design:
  - we only return one "average" metric:
     - as opposed to returning distribution and related metrics: variance, standard error, etc.
     - reasons:
        - users are mostly interested in this
        - simpler interface
        - distribution is only due to:
           - variation due to engine (garbage collection, optimization, etc.): not wanted in results
           - variation due to different input:
              - non-random set of inputs: should instead run several benchmarks with each set, instead of mixing several distributions
              - random set of inputs:
                 - want distribution of this??? E.g. load testing with real input
                 - could do the same logic as median but with percentile instead???
  - use medians instead of arithmetic mean:
     - reason: remove outliers due to:
        - garbage collection and background processes
        - engine optimization over time (first runs tend to be much slower)
  - use a recursive median of medians:
     - reason: O(log n) memory instead of O(n)
        - i.e. can have high number of iterations without crashing
     - use an outdegree of 3 because:
        - we want the smallest possible for minimum memory consumption
        - 2 would make an arithmetic mean instead of a median
  - remove the time taken to perform iteration itself:
     - reasons:
        - get current timestamp takes some time (45ns on my machine)
        - the iteration itself takes a little time:
           - time to increment counter (.3ns on my machine)
           - time to make function call, even if empty function (3ns on my machine)
     - i.e. allows precise measurement of nanoseconds-long functions
        - however if the function is as fast as the iteration itself (approx. <10ns), cannot measure because the iteration variance has too much influence.
           - the result then might be 0, and might vary a lot
  - if the task is very fast (approx. <0.1ms), also loop it and use arithmetic mean time:
     - reasons:
        - measured time must be too low compared to time resolution
           - we want at least 2 digits of precision
        - time taken to get current timestamp might be much slower than the task
           - i.e. much of the variance would be due to it, not to the task
           - we want task iteration to be at least 100 times the time required to get current timestamp
     - number of repetitions is automatically guessed
        - the same number is consistently used across the run
           - reason: looping a function might make it faster (due to engine optimizations)
     - alternative would be loop unrolling:
        - i.e. using `new Function('f', 'f();'.repeat(NUM))`:
        - pro: no bias of the iteration itself
        - cons:
           - requires lots of memory, i.e. max NUM would be required (probably 1e5, which is too low to estimate even ~50ns functions)
           - odd engine optimizations:
              - first call is much slower, i.e. requires a cold start
           - still need to calculate and remove the bias of calling that function itself (several nanoseconds bias)
           - cannot use a closure, i.e. function cannot reference upper scopes
  - use max duration:
     - as opposed to fixed number of loops
     - reasons:
        - faster code has higher variance:
           - does not increase variance as engines or code gets faster
           - when comparing fast tasks against slow tasks, make their variance more even
        - more stable user experience, avoiding potentially very long runs
        - get the most precision as user is ready to wait
        - no separate timeout feature needed
  - use bottom-up recursion:
     - reason: we don't know how many recursion levels are needed to reach max duration
  - run several child processes and get a median of them:
     - reasons:
        - when launching new processes (like when end user is running the command several times in CLI), variation is much
          higher than when using a single process and running several benchmarks in parallel
           - this is probably due to some engine optimization and global/OS variation
           - using a median over several child processes allow emulating user running those commands several times in CLI
     - we run them serially:
        - otherwise they start to compete with each other, i.e. time is slower and variance higher
     - we don't use worker threads because:
        - time slicing would skew measurements
        - only Node, not other platforms
        - they run in a slightly different environment (process.* has fewer methods)

Add lots of comments in the code, especially describing algorithm and constants

Task file should be loaded by child process
  - should be done during child process load right away, since this is done in parallel for all child processes, and it might be slow

OPTS.duration:
  - instead of OPTS.repeat
  - NUM (in secs)
  - def 10 secs
  - minimum 1 second
  - how long to benchmark each task
  - also used for timeout

Report mean duration not ops/sec:
  - use lowest unit, with minimum 10 (i.e. 2 digits)
  - integers only

Remove OPTS.tasks
  - reason: cannot be used with child processes since functions cannot be communicated to them
  - make OPTS.file a positional argument instead

Separate:
  - TASK.id:
     - used for stable identification
     - ESM exported variable name
  - TASK.name:
     - used for reporting
     - default to TASK.id

More input validation, including jest-validate

On thrown exceptions in main|before|after():
  - exit code 1
  - stop:
     - if OPTS.bail true (def: false), whole run
     - otherwise current main()+parameters repetitions
  - relies on p-times OPTS.stopOnError true (default value)
  - RESULT.failure OBJ:
     - error ERROR.stack
     - errorIndex NUM:
        - incrementing counter
        - used to display errors as footnotes during reporting
  - RESULT.failed BOOL: if any RESULT.failure set
  - try/catch block should include `performance.now()` so it does not skew timing
  - if child processes fail on load or has exit code, should bail whole run
     - ignore stderr???

OPTS.require "PATH"_ARR:
  - call import() (transpiled to require()) before loading OPTS.file
  - goal: babel/register, etc.

Progress bar:
  - for any reporter except silent
  - throttle to only update progress bar once per NUMms (find highest NUM without making it jittery when OPTS.repeat is high)

Reporters:
  - FUNC(RESULT_ARR, REPORTER_OPTS)[->STR]
     - if STR, either printed to console or put in file (if OPTS.replace)
  - OPTS.reporters 'MODULE'_ARR
  - REPORTER_OPTS:
     - passed by user as OPTS.REPORTEROPT, passed to FUNC as OPTS.OPT
  - types:
     - silent
     - JSON
     - CLI list
     - CLI table
     - Markdown list
     - Markdown table
  - CLI|Markdown list:
     - follow the formatting I used in fast-cartesian example
        - simple list for TASK with no TASK.parameters
  - CLI|Markdown tables:
     - parameters as x axis, mains as y axis
  - default reporter:
     - CLI table if more than half of cells would be filled, and some TASK.parameters are defined
     - CLI list otherwise
  - sort RESULT_ARR:
     - sort main() from fastest to slowest (using average of parameters)
     - sort parameters() from fastest to slowest (using average among main() with same name)

REPORTER_OPTS.output 'PATH':
  - def: "-" (stdout)
  - if file contains string "benchtap-start" and "benchtap-end", insert instead:
     - insert content as is, with no wrapping
     - agnostic to:
        - format of the file to modify
        - format of the inserted content
        - users need to pick ones that make sense, e.g. Markdown reporters for Markdown files
     - looks for delimiters:
        - "check-speed-start" and "check-speed-end"
           - remove anything in-between
          - require both. Reason: when moving it around the file, user should be aware that both should be moved
        - there can be other things on the line (e.g. <!-- COMMENT --> or #COMMENT), but whole line is kept
        - no parsing needed

Reporter showing advanced stats
  - e.g. graphs in terminal

OPTS.limit.TASK_ID NUM
  - maximum median for that task
  - if above:
     - exit code 1
     - RESULT.failure.limit true

Add a Gulp task to my gulp-shared-tasks

RESULT.system OBJ: hardware and software info

OPTS.platforms STR (use browserlist and PACKAGE.engines)
  - run in different browsers and Node versions
     - i.e. should probably enforce ESM
  - create one RESULT per platform, with different RESULT.system
  - abstract to own package
  - should allow adding platforms as plugins

Separate now.js to its own package

Separate resolution.js to its own package

Separate measuring part to its own package
  - child processes might be able to spawn only this instead of the full check-speed package

Promote:
  - add keywords (GitHub, package.json)

Commercial offer:
  - reporting dashboard:
     - show time series (i.e. keep history)
        - should not lose history when change only the `title` of the function|variant
     - nice data visualization
     - should show function bodies
  - code editor for benchmark files:
     - run benchmarks (inside browser not on our servers)
     - send benchmark results to API (like what users would do on CI otherwise)
     - show results from API
     - i.e. can be used like jsperf
  - Sharing like jsperf:
     - allow users to run in their own browsers
  - PR bot
  - notifications
  - user should report benchmark results to our API
     - like Codecov does
     - i.e. we do not pay for infrastructure cost, except simple CRUD API to store benchmark results
     - should be integrated with CI (i.e. use `ci-info`)
  - pricing:
     - free for open source
     - pay per private repo
