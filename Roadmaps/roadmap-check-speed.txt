
               
   CHECK-SPEED  
               



Loop bias:
  - once, before all runs
  - nowBias:
     - time taken to perform now()
     - calculated with getMedian(() => {}, 100ms)
        - repeat should always be 0
     - should be around 60ns on my machine
  - loopBias:
     - time taken to perform each loop in measureLoop()
     - calculated with getMedian(() => {}, 100ms)
        - repeat should always be 1e5
     - should be around .3ns on my machine
  - after each measure():
     - substract nowBias
     - substract repeats * loopBias
     - if result < 0 -> 1
     - except when calculating nowBias|loopBias themselves

Fast functions bias:
  - use `repeat` in measure():
     - initially 1
  - before each scale up:
     - after calculating median time
     - not if duration ran out
     - not if not `recurse`
  - then set `repeat` for next scale:
     - repeat = Math.ceil(min(A, B) / median)
        - A = time where only 1% is due to now(): 100 * timeOfNow
        - B = at least 2 digits of precision within time resolution: 100 * timeResolution
        - note: almost always A > B
        - timeOfNow, timeResolution and median should be in same unit (nanosecs)
  - when repeat changes, should discard current median (not pass it to next scale)???
     - find a way to prevent too many changes???
     - the bigger loopRepeat is, the more likely it is too change: i.e. should be less likely to trigger a discard???

Separate sync and async??? See differences once implemented

Remove `depth` argument from timing functions, unless it's now needed

Design:
  - we only return one "average" metric:
     - as opposed to returning distribution and related metrics: variance, standard error, etc.
     - reasons:
        - users are mostly interested in this
        - simpler interface
        - distribution is only due to:
           - variation due to engine (garbage collection, optimization, etc.): not wanted in results
           - variation due to different input:
              - non-random set of inputs: should instead run several benchmarks with each set, instead of mixing several distributions
              - random set of inputs:
                 - want distribution of this??? E.g. load testing with real input
                 - could do the same logic as median but with percentile instead???
  - use medians instead of arithmetic mean:
     - reason: remove outliers due to:
        - garbage collection and background processes
        - engine optimization over time (first runs tend to be much slower)
  - use a recursive median of medians:
     - reason: O(log n) memory instead of O(n)
        - i.e. can have high number of iterations without crashing
     - use an outdegree of 3 because:
        - we want the smallest possible for minimum memory consumption
        - 2 would make an arithmetic mean instead of a median
  - remove the time taken to perform iteration itself:
     - reasons:
        - get current timestamp takes some time (36ns on my machine)
        - the iteration itself takes a little time (1ns on my machine)
  - if the task is very fast (approx. <0.1ms), also loop it and use arithmetic mean time:
     - reasons:
        - measured time must be too low compared to time resolution
           - we want at least 2 digits of precision
        - time taken to get current timestamp might be much slower than the task
           - i.e. much of the variance would be due to it, not to the task
           - we want task iteration to be at least 100 times the time required to get current timestamp
     - number of repetitions is automatically guessed
        - the same number is consistently used across the run
           - reason: looping a function might make it faster (due to engine optimizations)
  - use max duration:
     - as opposed to fixed number of loops
     - reasons:
        - does not increase variance as engines or code gets faster
           - because faster code has higher variance
        - more stable user experience, avoiding potentially very long runs
        - get the most precision as user is ready to wait
        - no separate timeout feature needed
  - max duration is approximative:
     - reason: we need exactly 3 ** NUM iterations
     - this might be -|+ 50% of max duration (regardless of number of main())
        - average deviation is 25% if one task
           - minus 1/sqrt(1) if two tasks, then minus 1/sqrt(2) if three tasks, etc.
           - etc. 25% -> 16.7% -> 13.5% -> 11.7% -> 10.4%
  - use bottom-up recursion:
     - reason: we don't know how many recursion levels are needed to reach max duration

OPTS.duration:
  - instead of OPTS.repeat
  - def 10 secs
  - how long to benchmark each task
  - also used for timeout
  - NUM (in secs) or STR (ms())

Report mean duration not ops/sec:
  - use lowest unit, with minimum 10 (i.e. 2 digits)
  - integers only

More input validation, including jest-validate

Child process???
  - launch the measurement/iterated code in a child process
     - communicate using IPC channel
  - pros:
     - better with OPTS.platform
     - better with OPTS.require
     - less skewing with hot paths v8 optimization
         - should check if this is true
  - cons:
     - slower: check how much
     - not possible with OPTS.tasks: must use OPTS.file
  - if fail on load, should bail whole run
  - worker threads instead???
     - cons:
        - time slicing might skew measurements
        - only Node, not other platforms
        - process.* has fewer methods
        - Node 10+
  - how many to launch:
     - one per main+parameters???
     - one per CPU??? Split main+parameters into smaller parts if more CPUs???
  - not compatible with OPTS.tasks (only with OPTS.file)
  - divide OPTS.concurrency by number of processes???

On thrown exceptions in main|before|after():
  - exit code 1
  - stop:
     - if OPTS.bail true (def: false), whole run
     - otherwise current main()+parameters repetitions
  - relies on p-times OPTS.stopOnError true (default value)
  - RESULT.failure OBJ:
     - error ERROR.stack
     - errorIndex NUM:
        - incrementing counter
        - used to display errors as footnotes during reporting
  - RESULT.failed BOOL: if any RESULT.failure set
  - try/catch block should include `performance.now()` so it does not skew timing

OPTS.require "PATH"_ARR:
  - call import() (transpiled to require()) before loading OPTS.file
  - goal: babel/register, etc.

Progress bar:
  - for any reporter except silent
  - throttle to only update progress bar once per NUMms (find highest NUM without making it jittery when OPTS.repeat is high)

Reporters:
  - FUNC(RESULT_ARR, REPORTER_OPTS)[->STR]
     - if STR, either printed to console or put in file (if OPTS.replace)
  - OPTS.reporters 'MODULE'_ARR
  - REPORTER_OPTS:
     - passed by user as OPTS.REPORTEROPT, passed to FUNC as OPTS.OPT
  - types:
     - silent
     - JSON
     - CLI list
     - CLI table
     - Markdown list
     - Markdown table
  - CLI|Markdown list:
     - follow the formatting I used in fast-cartesian example
        - simple list for TASK with no TASK.parameters
  - CLI|Markdown tables:
     - parameters as x axis, mains as y axis
  - default reporter:
     - CLI table if more than half of cells would be filled, and some TASK.parameters are defined
     - CLI list otherwise
  - sort RESULT_ARR:
     - sort main() from fastest to slowest (using average of parameters)
     - sort parameters() from fastest to slowest (using average among main() with same name)

REPORTER_OPTS.output 'PATH':
  - def: "-" (stdout)
  - if file contains string "benchtap-start" and "benchtap-end", insert instead:
     - insert content as is, with no wrapping
     - agnostic to:
        - format of the file to modify
        - format of the inserted content
        - users need to pick ones that make sense, e.g. Markdown reporters for Markdown files
     - looks for delimiters:
        - "check-speed-start" and "check-speed-end"
           - remove anything in-between
          - require both. Reason: when moving it around the file, user should be aware that both should be moved
        - there can be other things on the line (e.g. <!-- COMMENT --> or #COMMENT), but whole line is kept
        - no parsing needed

Add a Gulp task to my gulp-shared-tasks

Add more stats in RESULT.duration:
  - mean NUM
  - average NUM
  - deviation NUM
  - variance NUM
  - min|max NUM
  - percentiles NUM_ARR
  - all NUM_ARR
  - margin of error, relative margin of error, standard error of mean

Reporter showing advanced stats
  - e.g. graphs in terminal

OPTS.thresholds OBJ:
  - same members as RESULT.duration
  - if above threshold:
     - exit code 1
     - on RESULT.failure.thresholds STR_ARR (e.g. ['mean'])

RESULT.system OBJ: hardware and software info

OPTS.platforms STR (use browserlist and PACKAGE.engines)
  - run in different browsers and Node versions
     - i.e. should probably enforce ESM
  - create one RESULT per platform, with different RESULT.system
  - abstract to own package
  - should allow adding platforms as plugins

Separate now.js to its own package

Separate resolution.js to its own package

Promote:
  - add keywords (GitHub, package.json)

Commercial offer:
  - reporting dashboard:
     - show time series (i.e. keep history)
        - should not lose history when change only the `title` of the function|variant
     - nice data visualization
     - should show function bodies
  - code editor for benchmark files:
     - run benchmarks (inside browser not on our servers)
     - send benchmark results to API (like what users would do on CI otherwise)
     - show results from API
     - i.e. can be used like jsperf
  - Sharing like jsperf:
     - allow users to run in their own browsers
  - PR bot
  - notifications
  - user should report benchmark results to our API
     - like Codecov does
     - i.e. we do not pay for infrastructure cost, except simple CRUD API to store benchmark results
     - should be integrated with CI (i.e. use `ci-info`)
  - pricing:
     - free for open source
     - pay per private repo
